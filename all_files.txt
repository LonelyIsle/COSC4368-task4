/home/wstewart/Desktop/COSC4368-task4/world.py
# world.py
# PD-World: 2-agent block transport environment
# State format: (i, j, i', j', x, x', a, b, c, d, e, f)
#   (i, j)   : Female agent position (row, col) in 1..GRID_SIZE
#   (i', j') : Male agent position (row, col)
#   x, x'    : Carry flags (0/1) for F and M
#   a, b, c  : Blocks at dropoffs (1,1), (1,5), (3,3)
#   d, e     : Blocks at the 2 pickup "slots" (mapped to current PICKUP_LOCATIONS[0/1])
#   f        : Blocks at dropoff (5,5)

import random
from typing import Tuple, Set, Dict, List

# ===========================================================
# World configuration
# ===========================================================

GRID_SIZE = 5  # valid rows/cols are 1..5 inclusive

# These coordinates can change during Exp 4 via set_pickups()
PICKUP_LOCATIONS: List[Tuple[int, int]] = [(3, 5), (4, 2)]

# Dropoffs are fixed
DROPOFF_LOCATIONS: List[Tuple[int, int]] = [(1, 1), (1, 5), (3, 3), (5, 5)]

# Capacities and initial counts
DROPOFF_CAPACITY = 5
INITIAL_PICKUP_BLOCKS = 10  # for both pickup slots at start

# Actions
ACTIONS = ['north', 'south', 'east', 'west', 'pickup', 'dropoff']

# Rewards
REWARD_MOVE = -1
REWARD_PICKUP = 13
REWARD_DROPOFF = 13


# ===========================================================
# State helpers
# ===========================================================

def get_initial_state() -> Tuple:
    """
    Initial state:
      F at (1,3), M at (5,3), both not carrying;
      dropoffs a,b,c,f start at 0;
      pickup-slot counts d,e start at 10 each.
    """
    # (i, j,  i', j',  x, x',  a, b, c,  d,  e,  f)
    return (1, 3,   5, 3,   0, 0,   0, 0, 0,  10, 10,  0)


def is_terminal_state(state: Tuple) -> bool:
    """
    Terminal when all dropoffs hit capacity: a=b=c=f=5.
    """
    _, _, _, _, _, _, a, b, c, _, _, f = state
    return (a == DROPOFF_CAPACITY
            and b == DROPOFF_CAPACITY
            and c == DROPOFF_CAPACITY
            and f == DROPOFF_CAPACITY)


def get_agent_position(state: Tuple, agent: str) -> Tuple[int, int]:
    """
    Return (row, col) for agent 'F' or 'M'.
    """
    i, j, i_p, j_p, *_ = state
    return (i, j) if agent == 'F' else (i_p, j_p)


def get_agent_carrying(state: Tuple, agent: str) -> int:
    """
    Return carry flag (0/1) for agent 'F' or 'M'.
    """
    _, _, _, _, x, x_p, *_ = state
    return x if agent == 'F' else x_p


def get_block_counts(state: Tuple) -> Dict[str, int]:
    """
    Human-readable dict of block counts at sites.
    """
    *_, a, b, c, d, e, f = state
    return {
        'dropoff_1_1': a,
        'dropoff_1_5': b,
        'dropoff_3_3': c,
        'pickup_slot0': d,  # mapped to PICKUP_LOCATIONS[0]
        'pickup_slot1': e,  # mapped to PICKUP_LOCATIONS[1]
        'dropoff_5_5': f,
    }


# ===========================================================
# Action applicability
# ===========================================================

def aplop(state: Tuple, agent: str) -> Set[str]:
    """
    Applicable operators for agent in current state.
    Enforces:
      - 1..GRID_SIZE boundaries
      - no occupying same cell
      - pickup only at pickup cells with stock and not carrying
      - dropoff only at dropoff cells with capacity and carrying
    """
    i, j, i_p, j_p, x, x_p, a, b, c, d, e, f = state

    if agent == 'F':
        r, col = i, j
        carrying = x
        other_r, other_c = i_p, j_p
    else:
        r, col = i_p, j_p
        carrying = x_p
        other_r, other_c = i, j

    ops: Set[str] = set()

    # Move north
    if r > 1 and not (r - 1 == other_r and col == other_c):
        ops.add('north')
    # Move south
    if r < GRID_SIZE and not (r + 1 == other_r and col == other_c):
        ops.add('south')
    # Move east
    if col < GRID_SIZE and not (r == other_r and col + 1 == other_c):
        ops.add('east')
    # Move west
    if col > 1 and not (r == other_r and col - 1 == other_c):
        ops.add('west')

    # Pickup (dynamic positions)
    if carrying == 0:
        if (r, col) == tuple(PICKUP_LOCATIONS[0]) and d > 0:
            ops.add('pickup')
        elif (r, col) == tuple(PICKUP_LOCATIONS[1]) and e > 0:
            ops.add('pickup')

    # Dropoff (fixed positions)
    if carrying == 1:
        if (r, col) == (1, 1) and a < DROPOFF_CAPACITY:
            ops.add('dropoff')
        elif (r, col) == (1, 5) and b < DROPOFF_CAPACITY:
            ops.add('dropoff')
        elif (r, col) == (3, 3) and c < DROPOFF_CAPACITY:
            ops.add('dropoff')
        elif (r, col) == (5, 5) and f < DROPOFF_CAPACITY:
            ops.add('dropoff')

    return ops


# ===========================================================
# Transition
# ===========================================================

def apply(state: Tuple, action: str, agent: str) -> Tuple[Tuple, int]:
    """
    Apply action for the given agent. Assumes action is applicable.
    Returns (new_state, reward).
    """
    i, j, i_p, j_p, x, x_p, a, b, c, d, e, f = state

    # copy locals
    new_i, new_j = i, j
    new_i_p, new_j_p = i_p, j_p
    new_x, new_x_p = x, x_p
    new_a, new_b, new_c, new_d, new_e, new_f = a, b, c, d, e, f

    # Current agent snapshot
    if agent == 'F':
        r, col = i, j
        carrying = x
    else:
        r, col = i_p, j_p
        carrying = x_p

    reward = 0

    if action == 'north':
        if agent == 'F':
            new_i = r - 1
        else:
            new_i_p = r - 1
        reward = REWARD_MOVE

    elif action == 'south':
        if agent == 'F':
            new_i = r + 1
        else:
            new_i_p = r + 1
        reward = REWARD_MOVE

    elif action == 'east':
        if agent == 'F':
            new_j = col + 1
        else:
            new_j_p = col + 1
        reward = REWARD_MOVE

    elif action == 'west':
        if agent == 'F':
            new_j = col - 1
        else:
            new_j_p = col - 1
        reward = REWARD_MOVE

    elif action == 'pickup':
        if agent == 'F':
            new_x = 1
        else:
            new_x_p = 1

        # Decrement whichever pickup slot matches current coordinate
        if (r, col) == tuple(PICKUP_LOCATIONS[0]):
            new_d = d - 1
        elif (r, col) == tuple(PICKUP_LOCATIONS[1]):
            new_e = e - 1

        reward = REWARD_PICKUP

    elif action == 'dropoff':
        if agent == 'F':
            new_x = 0
        else:
            new_x_p = 0

        if (r, col) == (1, 1):
            new_a = a + 1
        elif (r, col) == (1, 5):
            new_b = b + 1
        elif (r, col) == (3, 3):
            new_c = c + 1
        elif (r, col) == (5, 5):
            new_f = f + 1

        reward = REWARD_DROPOFF

    new_state = (
        new_i, new_j, new_i_p, new_j_p,
        new_x, new_x_p, new_a, new_b, new_c,
        new_d, new_e, new_f
    )
    return new_state, reward


# ===========================================================
# Adapters for experiment runners
# ===========================================================

def reset(seed: int = 0):
    """Reset world and return the initial state. Seed accepted for compatibility."""
    random.seed(seed)
    return get_initial_state()


def applicable_actions(state: Tuple, agent: str):
    """Adapter name expected by runners; wraps aplop()."""
    return aplop(state, agent)


def step(state: Tuple, action: str, agent: str):
    """
    One environment step for agent. Returns (next_state, reward, done).
    """
    next_state, reward = apply(state, action, agent)
    done = is_terminal_state(next_state)
    return next_state, reward, done


def set_pickups(pickups):
    """
    Experiment 4 hook: change pickup coordinates WITHOUT touching d/e counts in the state.
    The two slot counts (d,e) always map to PICKUP_LOCATIONS[0] and [1], respectively.
    """
    global PICKUP_LOCATIONS
    if not isinstance(pickups, (list, tuple)) or len(pickups) != 2:
        raise ValueError("set_pickups expects exactly two pickup coordinates, e.g., [(1,2),(4,5)]")
    PICKUP_LOCATIONS = [tuple(pickups[0]), tuple(pickups[1])]
/home/wstewart/Desktop/COSC4368-task4/main.py
# main.py
# Entry point for running Task 4 experiments.
# You can run: python main.py --exp 1b | 1c | 1d | 2 | 3q | 3sarsa | 4q | 4sarsa

import argparse
from experiments import (
    two_runs_exp1_prandom,
    two_runs_exp1_pgreedy,
    two_runs_exp1_pexploit,
    two_runs_exp2,
    two_runs_exp3,
    two_runs_exp4,
    summarize_run,
)
from visualize import (
    plot_world,
    plot_episode_trace,
    plot_distance_over_time,
    plot_q_arrows,
)
import world as W

def run_and_summarize(experiment_fn, seedA, seedB, label):
    print(f"\n{'=' * 20} {label} {'=' * 20}")
    rA, rB = experiment_fn(seedA, seedB)
    print("Run A Summary:")
    print(summarize_run(rA))
    print("\nRun B Summary:")
    print(summarize_run(rB))
    return rA, rB

def visualize_results(run, grid_size=(5, 5)):
    # Visualize world state & learned Q
    print("\nGenerating visuals for one sample run...")

    if run.episodes:
        # Mock a sample trace (you can log real episode_states if you want more detail)
        state = W.get_initial_state()
        plot_world(
            state,
            grid_size=grid_size,
            pickup_cells=W.PICKUP_LOCATIONS,
            dropoff_cells=W.DROPOFF_LOCATIONS,
            title="Initial World State"
        )

    # Q-table visualization
    plot_q_arrows(
        run.Q,
        grid_size=grid_size,
        for_agent='F',
        title="Greedy Policy Arrows (F)"
    )
    plot_q_arrows(
        run.Q,
        grid_size=grid_size,
        for_agent='M',
        title="Greedy Policy Arrows (M)"
    )
    print("Visuals displayed successfully.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run Task 4 experiments.")
    parser.add_argument("--exp", required=True,
                        help="1b | 1c | 1d | 2 | 3q | 3sarsa | 4q | 4sarsa")
    parser.add_argument("--seedA", type=int, default=7)
    parser.add_argument("--seedB", type=int, default=19)
    args = parser.parse_args()

    exp_map = {
        "1b": (two_runs_exp1_prandom, "Experiment 1(b) - Q-Learning PRANDOM"),
        "1c": (two_runs_exp1_pgreedy, "Experiment 1(c) - Q-Learning PGREEDY"),
        "1d": (two_runs_exp1_pexploit, "Experiment 1(d) - Q-Learning PEXPLOIT"),
        "2": (two_runs_exp2, "Experiment 2 - SARSA (PGREEDY)"),
        "3q": (lambda a, b: two_runs_exp3(base="q", seed_a=a, seed_b=b), "Experiment 3 - Q-Learning (lower Î±, Î³)"),
        "3sarsa": (lambda a, b: two_runs_exp3(base="sarsa", seed_a=a, seed_b=b), "Experiment 3 - SARSA (lower Î±, Î³)"),
        "4q": (lambda a, b: two_runs_exp4(base="q", seed_a=a, seed_b=b), "Experiment 4 - Q-Learning Pickup Switch"),
        "4sarsa": (lambda a, b: two_runs_exp4(base="sarsa", seed_a=a, seed_b=b), "Experiment 4 - SARSA Pickup Switch"),
    }

    if args.exp not in exp_map:
        print("Unknown experiment ID. Use one of: 1b | 1c | 1d | 2 | 3q | 3sarsa | 4q | 4sarsa")
        exit(1)

    exp_fn, label = exp_map[args.exp]
    resultA, resultB = run_and_summarize(exp_fn, args.seedA, args.seedB, label)

    # Optional: visualize one run
    visualize_results(resultA)
/home/wstewart/Desktop/COSC4368-task4/policies.py
# policies.py
from typing import Iterable, Dict, Tuple, List
import random
import math

State = Tuple  # your state tuple

def _as_list(actions: Iterable[str]) -> List[str]:
    lst = list(actions)
    if not lst:
        raise ValueError("No applicable actions available.")
    return lst

def _q(Q: Dict[Tuple[State, str], float], state: State, action: str) -> float:
    # experiments.py stores Q with keys: (state, action)
    return Q.get((state, action), 0.0)

def _best_action(Q: Dict[Tuple[State, str], float], state: State, actions: Iterable[str]) -> str:
    acts = _as_list(actions)
    best_q = -math.inf
    best: List[str] = []
    for a in acts:
        q = _q(Q, state, a)
        if q > best_q:
            best_q = q
            best = [a]
        elif q == best_q:
            best.append(a)
    return random.choice(best) if best else random.choice(acts)

# ---------------------------
# Policies used by the project
# ---------------------------

def prandom(state: State, applicable_actions: Iterable[str], q_table: Dict) -> str:
    """
    PRANDOM: If pickup or dropoff is applicable, do it; otherwise choose random.
    """
    acts = _as_list(applicable_actions)
    if 'pickup' in acts:
        return 'pickup'
    if 'dropoff' in acts:
        return 'dropoff'
    return random.choice(acts)

def pgreedy(state: State, applicable_actions: Iterable[str], q_table: Dict) -> str:
    """
    PGREEDY: If pickup/dropoff is applicable, do it; otherwise choose argmax-Q (break ties randomly).
    """
    acts = _as_list(applicable_actions)
    if 'pickup' in acts:
        return 'pickup'
    if 'dropoff' in acts:
        return 'dropoff'
    return _best_action(q_table, state, acts)

def pexploit(state: State, applicable_actions: Iterable[str], q_table: Dict, epsilon: float = 0.2) -> str:
    """
    PEXPLOIT: If pickup/dropoff is applicable, do it; otherwise:
      - with probability (1 - epsilon) pick greedy action
      - with probability epsilon pick a random applicable action
    Default epsilon=0.2 matches spec's 80/20 split.
    """
    acts = _as_list(applicable_actions)
    if 'pickup' in acts:
        return 'pickup'
    if 'dropoff' in acts:
        return 'dropoff'
    if random.random() < epsilon:
        return random.choice(acts)
    return _best_action(q_table, state, acts)

# ---------------------------
# Backward-compatible aliases (your older code can still call these)
# ---------------------------

def PRandom(state: State, applicable_actions: Iterable[str], q_table: Dict) -> str:
    return prandom(state, applicable_actions, q_table)

def PGreedy(state: State, applicable_actions: Iterable[str], q_table: Dict) -> str:
    return pgreedy(state, applicable_actions, q_table)

def PExploit(state: State, applicable_actions: Iterable[str], q_table: Dict, epsilon: float = 0.2) -> str:
    return pexploit(state, applicable_actions, q_table, epsilon=epsilon)
/home/wstewart/Desktop/COSC4368-task4/q_learning.py
"""
Everything in this file is related to Q_learning
contains the algorithm and the means to update tables

for reference:
State format: (i, j, i', j', x, x', a, b, c, d, e, f)
    - (i,j): Female agent position
    - (i',j'): Male agent position
    - x: Female carrying block (0 or 1)
    - x': Male carrying block (0 or 1)
    - a: blocks in dropoff (1,1)
    - b: blocks in dropoff (1,5)
    - c: blocks in dropoff (3,3)
    - d: blocks in pickup (3,5)
    - e: blocks in pickup (4,2)
    - f: blocks in dropoff (5,5)
"""

import numpy as np
import random
from typing import Tuple, Set, Dict

def create_q_table():
    return {}


def set_q_values(q_table: Dict, state: Tuple, action: str, value: float):
    """
    q_table: Dictionary storing Q-values
    state: Simplified state tuple
    action: Action string
    value: New Q-value
    """

    key = (state, action)
    q_table[key] = value

def get_q_values(q_table: Dict, state: Tuple, action: str) -> float:
    """
    q_table: Dictionary storing q-values
    state: Simplified state tuple (i, j, x, delta_i, delta_j)
    action: Action string
    """

    key = (state, action)
    return q_table.get(key, 0.0) # Return key or 0

def get_best_action(q_table: Dict, state: Tuple, applicable_actions: Set[str]) -> str:
    """
    Return action with highest Q-value from applicable actions.
    Breaks ties randomly.

    q_table: Dictionary storing Q-values
    state: Simplified state tuple
    applicable_actions: Set of valid actions
    """
    # Get q-values for all applicable actions
    action_q_vals = {}
    for action in applicable_actions:
        q_value = get_q_values(q_table, state, action)
        action_q_vals[action] = q_value

    # find the max q_value
    max_value = max(action_q_vals.values())

    # tie-breaker function - if multiple actions have the same value select at random
    best_action = []
    for action, q_value in action_q_vals.items():
        if q_value == max_value:
            best_action.append(action)

    return random.choice(best_action) # randomly pick action, not an issue if only one option


def get_max_q_value(q_table: Dict, state: Tuple, applicable_actions: Set[str]) -> float:
    """
    Return the highest Q-value among applicable actions.
    this function is for returning the number for q-learning formula

    q_table: Dictionary storing Q-values
    state: Simplified state tuple
    applicable_actions: Set of valid actions

    """
    # Safety check
    if not applicable_actions:
        return 0.0

    # Get Q-values for all actions
    q_values = []
    for action in applicable_actions:
        q_value = get_q_values(q_table, state, action)
        q_values.append(q_value)

    # Return the maximum
    return max(q_values)

def simplify_state(full_state: Tuple, agent: str) -> Tuple:
    """
    simplify the state to only use what is needed for the q-learning algorithm
    refer to the top of this code to see what the full state is and what variables mean what
    simplified state space : (i, j, x, delta_i, delta_j) is the end result
        - (i, j) = agent position
        - x = agent carrying block (boolean)
        - delta_i, delta_j = Relative row/column position to other agent
    """
    i, j, i_prime, j_prime, x, x_prime, a, b, c, d, e, f = full_state

    # determine which agent we are dealing with
    if agent == 'F': # Female agent
        delta_i = i - i_prime
        delta_j = j - j_prime
        return i, j, x, delta_i, delta_j
    else: # Male agent
        delta_i = i_prime - i
        delta_j = j_prime - j
        return i_prime, j_prime, x_prime, delta_i, delta_j

def update_q_learning(q_table: Dict,
                     state: Tuple,
                     action: str,
                     reward: float,
                     next_state: Tuple,
                     applicable_next_actions: Set[str],
                     alpha: float,
                     gamma: float):
    """
    Formula: Q(s,a) â† (1-Î±)*Q(s,a) + Î±*[R + Î³*max Q(s',a')]

    q_table: Dictionary storing Q-values
        state: Current simplified state
        action: Action taken
        reward: Immediate reward received
        next_state: Next simplified state
        applicable_next_actions: Set of valid actions in next state
        alpha: Learning rate (0 < Î± â‰¤ 1)
        gamma: Discount factor (0 â‰¤ Î³ â‰¤ 1)

        note from slides:
            "aâ€™ has to be an applicable operator in sâ€™; e.g. pickup and drop-off are not
            applicable in a pickup/dropoff states if empty/full! The q-values of
            non-applicable operators are therefore not considered! "
    """
    old_q = get_q_values(q_table, state, action)

    # Get max q-value in next state (ONLY from the applicable actions!)
    max_next_q = get_max_q_value(q_table, next_state, applicable_next_actions)

    # Calculate new q-value using Q-Learning formula
    new_q = (1 - alpha) * old_q + alpha * (reward + gamma * max_next_q)

    # Update the q-table
    set_q_values(q_table, state, action, new_q)


/home/wstewart/Desktop/COSC4368-task4/flattener.py
#!/usr/bin/env python3
"""
flattener.py

Put this script in the root directory you want to scan, then run:

    python flattener.py

It will create (or overwrite) all_files.txt in the same directory.
"""

import os

def write_all_files_here(out_name: str = "all_files.txt", include_hidden: bool = False) -> None:
    root_dir = os.path.dirname(os.path.abspath(__file__))
    out_path = os.path.join(root_dir, out_name)

    with open(out_path, "w", encoding="utf-8", errors="replace") as out_f:
        for dirpath, dirnames, filenames in os.walk(root_dir):
            # Skip hidden directories and files unless requested
            if not include_hidden:
                dirnames[:] = [d for d in dirnames if not d.startswith('.')]
                filenames = [f for f in filenames if not f.startswith('.')]

            for fname in filenames:
                file_path = os.path.join(dirpath, fname)
                # Skip the output file itself
                if os.path.abspath(file_path) == os.path.abspath(out_path):
                    continue

                # Write the file path
                out_f.write(f"{file_path}\n")

                # Try to read the file as text
                try:
                    with open(file_path, "r", encoding="utf-8", errors="replace") as f:
                        for line in f:
                            out_f.write(line)
                except Exception as e:
                    out_f.write(f"<ERROR: cannot read file: {e}>\n")

                out_f.write("\n")  # blank line between entries

    print(f"Wrote flattened file list to: {out_path}")


if __name__ == "__main__":
    write_all_files_here()
/home/wstewart/Desktop/COSC4368-task4/visualize.py
# visualize.py
# Matplotlib-only helpers for world snapshots, episode traces, distance, and Q arrows.
# IMPORTANT: One chart per function (no subplots), no seaborn, no custom colors.

from typing import Dict, Iterable, List, Tuple, Callable, Any
import math
import matplotlib.pyplot as plt

GridPos = Tuple[int, int]
State = Tuple  # (i, j, i', j', x, x', a, b, c, d, e, f)

# --- You can tweak these defaults if needed ---
DEFAULT_GRID = (5, 5)

def extract_positions_from_state(state: State) -> Tuple[GridPos, GridPos]:
    # Matches your state: (i, j, i', j', ...)
    return (int(state[0]), int(state[1])), (int(state[2]), int(state[3]))

def _draw_grid(ax, width: int, height: int):
    ax.set_xlim(0.5, width + 0.5)
    ax.set_ylim(0.5, height + 0.5)
    ax.set_xticks(range(1, width + 1))
    ax.set_yticks(range(1, height + 1))
    ax.grid(True)
    ax.set_aspect('equal', adjustable='box')
    ax.invert_yaxis()
    ax.set_xlabel('col')
    ax.set_ylabel('row')

def plot_world(
    state: State,
    grid_size: Tuple[int, int] = DEFAULT_GRID,
    pickup_cells: Iterable[GridPos] = ((3, 5), (4, 2)),
    dropoff_cells: Iterable[GridPos] = ((1, 1), (1, 5), (3, 3), (5, 5)),
    position_extractor: Callable[[State], Tuple[GridPos, GridPos]] = extract_positions_from_state,
    title: str = 'PD-World snapshot'
):
    width, height = grid_size
    f_pos, m_pos = position_extractor(state)

    fig, ax = plt.subplots()
    _draw_grid(ax, width, height)

    for (x, y) in pickup_cells:
        ax.scatter([x], [y], marker='s', s=200, label='pickup')
    for (x, y) in dropoff_cells:
        ax.scatter([x], [y], marker='D', s=200, label='dropoff')

    ax.scatter([f_pos[1]], [f_pos[0]], marker='o', s=200, label='F')  # (row,col) -> (y,x)
    ax.scatter([m_pos[1]], [m_pos[0]], marker='^', s=200, label='M')

    ax.legend(loc='upper right')
    ax.set_title(title)
    plt.show()

def plot_episode_trace(
    states: List[State],
    grid_size: Tuple[int, int] = DEFAULT_GRID,
    pickup_cells: Iterable[GridPos] = ((3, 5), (4, 2)),
    dropoff_cells: Iterable[GridPos] = ((1, 1), (1, 5), (3, 3), (5, 5)),
    position_extractor: Callable[[State], Tuple[GridPos, GridPos]] = extract_positions_from_state,
    title: str = 'Episode trace (agent paths)'
):
    width, height = grid_size
    fig, ax = plt.subplots()
    _draw_grid(ax, width, height)

    f_path: List[GridPos] = []
    m_path: List[GridPos] = []
    for s in states:
        f_pos, m_pos = position_extractor(s)
        f_path.append(f_pos); m_path.append(m_pos)

    for (x, y) in pickup_cells:
        ax.scatter([x], [y], marker='s', s=120)
    for (x, y) in dropoff_cells:
        ax.scatter([x], [y], marker='D', s=120)

    # paths: convert (row,col) -> (x=col, y=row)
    if len(f_path) >= 2:
        xs = [p[1] for p in f_path]; ys = [p[0] for p in f_path]
        ax.plot(xs, ys, linewidth=2, label='F path')
    if len(m_path) >= 2:
        xs = [p[1] for p in m_path]; ys = [p[0] for p in m_path]
        ax.plot(xs, ys, linewidth=2, linestyle='--', label='M path')

    if f_path:
        ax.scatter([f_path[0][1]], [f_path[0][0]], s=100, label='F start')
        ax.scatter([f_path[-1][1]], [f_path[-1][0]], s=100, marker='x', label='F end')
    if m_path:
        ax.scatter([m_path[0][1]], [m_path[0][0]], s=100, label='M start')
        ax.scatter([m_path[-1][1]], [m_path[-1][0]], s=100, marker='x', label='M end')

    ax.legend(loc='upper right')
    ax.set_title(title)
    plt.show()

def plot_distance_over_time(
    states: List[State],
    position_extractor: Callable[[State], Tuple[GridPos, GridPos]] = extract_positions_from_state,
    title: str = 'Agent Manhattan distance over time'
):
    distances: List[int] = []
    for s in states:
        (fr, fc), (mr, mc) = position_extractor(s)
        distances.append(abs(fr - mr) + abs(fc - mc))

    fig, ax = plt.subplots()
    ax.plot(range(len(distances)), distances)
    ax.set_xlabel('Step')
    ax.set_ylabel('Manhattan distance')
    ax.set_title(title)
    plt.show()

def _arrow_for_action(action: str) -> Tuple[float, float]:
    if action == 'north':  return (0, -0.3)
    if action == 'south':  return (0,  0.3)
    if action == 'west':   return (-0.3, 0)
    if action == 'east':   return (0.3,  0)
    return (0.0, 0.0)  # pickup/dropoff/none

def plot_q_arrows(
    q_table: Dict[Tuple[Any, str], float],
    grid_size: Tuple[int, int] = DEFAULT_GRID,
    for_agent: str = 'F',
    position_extractor: Callable[[State], Tuple[GridPos, GridPos]] = extract_positions_from_state,
    title: str = 'Greedy action by cell from Q-table',
    show_values: bool = True
):
    # Aggregate across all states that place the chosen agent at (row,col)
    width, height = grid_size
    fig, ax = plt.subplots()
    _draw_grid(ax, width, height)

    # collect best (action, q) per cell
    best_per_cell: Dict[GridPos, Tuple[str, float]] = {}
    for (state, action), q in q_table.items():
        f_pos, m_pos = position_extractor(state)
        row, col = (f_pos if for_agent == 'F' else m_pos)
        cell = (row, col)
        if cell not in best_per_cell or q > best_per_cell[cell][1]:
            best_per_cell[cell] = (action, q)

    for (row, col), (action, q) in best_per_cell.items():
        dx, dy = _arrow_for_action(action)
        # convert to plotting coords: (x=col, y=row); dy sign is handled by inverted axis
        ax.arrow(col, row, dx, dy, head_width=0.12, length_includes_head=True)
        if show_values and math.isfinite(q):
            ax.text(col, row, f"{q:.2f}", ha='center', va='center')

    ax.set_title(title + f" (agent={for_agent})")
    plt.show()
/home/wstewart/Desktop/COSC4368-task4/experiments.py
# experiments.py
# Runners for Task 4 Experiments (1â€“4), plus Q-learning & SARSA updates.
# Works with:
#   - world.py exposing: reset(seed), applicable_actions(state,agent), step(state,action,agent), set_pickups(pickups)
#   - policies.py exposing: prandom, pgreedy, pexploit

import random
import math
from dataclasses import dataclass
from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple

# ---- Bind to your project modules ----
import world as W
import policies as P

# ---- Types ----
State = Tuple
QTable = Dict[Tuple[Any, str], float]

# ---- Q helpers ----
def q_get(Q: QTable, s: State, a: str) -> float:
    return Q.get((s, a), 0.0)

def q_set(Q: QTable, s: State, a: str, v: float) -> None:
    Q[(s, a)] = v

def argmax_q(Q: QTable, state: State, actions: Sequence[str]) -> str:
    best_q = -math.inf
    best = []
    for a in actions:
        q = q_get(Q, state, a)
        if q > best_q:
            best_q = q; best = [a]
        elif q == best_q:
            best.append(a)
    return random.choice(best) if best else random.choice(list(actions))

# ---- Updates ----
def update_qlearning(Q: QTable, s: State, a: str, r: float, s_next: State, alpha: float, gamma: float,
                     next_actions: Sequence[str]) -> None:
    target = r + (gamma * max(q_get(Q, s_next, an) for an in next_actions)) if next_actions else r
    new_q = (1 - alpha) * q_get(Q, s, a) + alpha * target
    q_set(Q, s, a, new_q)

def update_sarsa(Q: QTable, s: State, a: str, r: float, s_next: State, a_next: Optional[str],
                 alpha: float, gamma: float) -> None:
    target = r + (gamma * q_get(Q, s_next, a_next) if a_next is not None else 0.0)
    new_q = (1 - alpha) * q_get(Q, s, a) + alpha * target
    q_set(Q, s, a, new_q)

# ---- Metrics ----
def manhattan_between_agents(s: State) -> int:
    # Your state: (i, j, i', j', x, x', a, b, c, d, e, f)
    return abs(s[0] - s[2]) + abs(s[1] - s[3])

@dataclass
class EpisodeLog:
    steps: int
    total_reward: float
    avg_manhattan: float

@dataclass
class RunResult:
    seed: int
    total_steps: int
    terminals_reached: int
    episodes: List[EpisodeLog]
    Q: QTable  # final Q

# ---- Core rollout ----
def rollout(
    seed: int,
    total_steps: int,
    warmup_steps: int,
    alpha: float,
    gamma: float,
    learner: str,                 # 'q' or 'sarsa'
    schedule_after_warmup: str,   # 'prandom' | 'pgreedy' | 'pexploit'
) -> RunResult:
    random.seed(seed)
    state = W.reset(seed)
    Q: QTable = {}
    current_agent = 'F'  # Female starts
    step_count = 0
    terminals = 0

    # episode buffers
    episode_states: List[State] = [state]
    episode_rewards = 0.0
    episode_steps = 0
    episode_dist_sum = 0
    episodes: List[EpisodeLog] = []

    def pick_policy():
        if step_count < warmup_steps:
            return P.prandom
        return {'prandom': P.prandom, 'pgreedy': P.pgreedy, 'pexploit': P.pexploit}.get(schedule_after_warmup, P.pgreedy)

    while step_count < total_steps:
        actions = W.applicable_actions(state, current_agent)
        if not actions:
            current_agent = 'M' if current_agent == 'F' else 'F'
            continue

        policy = pick_policy()
        action = policy(state, actions, Q)

        next_state, reward, done = W.step(state, action, current_agent)
        step_count += 1
        episode_steps += 1
        episode_rewards += reward
        episode_dist_sum += manhattan_between_agents(next_state)
        episode_states.append(next_state)

        if learner == 'q':
            next_actions = W.applicable_actions(next_state, 'M' if current_agent == 'F' else 'F')
            update_qlearning(Q, state, action, reward, next_state, alpha, gamma, next_actions)
        else:
            next_actions_curr = W.applicable_actions(next_state, 'M' if current_agent == 'F' else 'F')
            a_next = None
            if next_actions_curr:
                a_next = pick_policy()(next_state, next_actions_curr, Q)
            update_sarsa(Q, state, action, reward, next_state, a_next, alpha, gamma)

        state = next_state
        current_agent = 'M' if current_agent == 'F' else 'F'

        if done:
            terminals += 1
            avg_manh = episode_dist_sum / max(1, episode_steps)
            episodes.append(EpisodeLog(steps=episode_steps, total_reward=episode_rewards, avg_manhattan=avg_manh))
            # new episode, keep Q-table
            state = W.reset(seed + terminals)   # small perturbation per episode
            episode_states = [state]
            episode_rewards = 0.0
            episode_steps = 0
            episode_dist_sum = 0
            current_agent = 'F'

    return RunResult(seed=seed, total_steps=step_count, terminals_reached=terminals, episodes=episodes, Q=Q)

# ---- Experiments ----
def run_experiment1_variant(variant: str, seed: int) -> RunResult:
    # Î±=0.3, Î³=0.5, 8000 steps, warmup 500; variant = 'prandom'| 'pgreedy' | 'pexploit'
    return rollout(seed=seed, total_steps=8000, warmup_steps=500,
                   alpha=0.3, gamma=0.5, learner='q', schedule_after_warmup=variant)

def run_experiment2(seed: int) -> RunResult:
    # SARSA; same as 1(c): post-warmup PGREEDY
    return rollout(seed=seed, total_steps=8000, warmup_steps=500,
                   alpha=0.3, gamma=0.5, learner='sarsa', schedule_after_warmup='pgreedy')

def run_experiment3(seed: int, base: str = 'q') -> RunResult:
    # like 1(c) if base='q' or like 2 if base='sarsa' but Î±=0.15, Î³=0.45
    learner = 'q' if base == 'q' else 'sarsa'
    return rollout(seed=seed, total_steps=8000, warmup_steps=500,
                   alpha=0.15, gamma=0.45, learner=learner, schedule_after_warmup='pgreedy')

def run_experiment4(seed: int, base: str = 'q') -> RunResult:
    # Start like 1(c) or 2; after 3rd terminal, set_pickups([(1,2),(4,5)]); continue to 6th
    random.seed(seed)
    state = W.reset(seed)
    Q: QTable = {}
    current_agent = 'F'
    step_count = 0
    terminals = 0

    alpha, gamma = 0.3, 0.5
    warmup_steps = 500
    learner = 'q' if base == 'q' else 'sarsa'

    episode_states: List[State] = [state]
    episode_rewards = 0.0
    episode_steps = 0
    episode_dist_sum = 0
    episodes: List[EpisodeLog] = []

    def pick_policy():
        if step_count < warmup_steps:
            return P.prandom
        return P.pgreedy

    # generous cap to avoid infinite loops
    while terminals < 6 and step_count < 20000:
        actions = W.applicable_actions(state, current_agent)
        if not actions:
            current_agent = 'M' if current_agent == 'F' else 'F'
            continue

        action = pick_policy()(state, actions, Q)
        next_state, reward, done = W.step(state, action, current_agent)
        step_count += 1
        episode_steps += 1
        episode_rewards += reward
        episode_dist_sum += manhattan_between_agents(next_state)
        episode_states.append(next_state)

        if learner == 'q':
            next_actions = W.applicable_actions(next_state, 'M' if current_agent == 'F' else 'F')
            update_qlearning(Q, state, action, reward, next_state, alpha, gamma, next_actions)
        else:
            next_actions_curr = W.applicable_actions(next_state, 'M' if current_agent == 'F' else 'F')
            a_next = None
            if next_actions_curr:
                a_next = pick_policy()(next_state, next_actions_curr, Q)
            update_sarsa(Q, state, action, reward, next_state, a_next, alpha, gamma)

        state = next_state
        current_agent = 'M' if current_agent == 'F' else 'F'

        if done:
            terminals += 1
            avg_manh = episode_dist_sum / max(1, episode_steps)
            episodes.append(EpisodeLog(steps=episode_steps, total_reward=episode_rewards, avg_manhattan=avg_manh))

            if terminals == 3:
                W.set_pickups([(1, 2), (4, 5)])  # change pickups, keep Q

            state = W.reset(seed + terminals)
            episode_states = [state]
            episode_rewards = 0.0
            episode_steps = 0
            episode_dist_sum = 0
            current_agent = 'F'

    return RunResult(seed=seed, total_steps=step_count, terminals_reached=terminals, episodes=episodes, Q=Q)

# ---- Two-run helpers ----
def two_runs(fn_runner: Callable[[int], RunResult], seed_a: int, seed_b: int):
    return fn_runner(seed_a), fn_runner(seed_b)

def two_runs_exp1_prandom(seed_a: int = 7, seed_b: int = 19):
    return two_runs(lambda s: run_experiment1_variant('prandom', s), seed_a, seed_b)

def two_runs_exp1_pgreedy(seed_a: int = 7, seed_b: int = 19):
    return two_runs(lambda s: run_experiment1_variant('pgreedy', s), seed_a, seed_b)

def two_runs_exp1_pexploit(seed_a: int = 7, seed_b: int = 19):
    return two_runs(lambda s: run_experiment1_variant('pexploit', s), seed_a, seed_b)

def two_runs_exp2(seed_a: int = 11, seed_b: int = 23):
    return two_runs(run_experiment2, seed_a, seed_b)

def two_runs_exp3(base: str = 'q', seed_a: int = 13, seed_b: int = 29):
    return two_runs(lambda s: run_experiment3(s, base=base), seed_a, seed_b)

def two_runs_exp4(base: str = 'q', seed_a: int = 17, seed_b: int = 31):
    return two_runs(lambda s: run_experiment4(s, base=base), seed_a, seed_b)

# ---- Pretty summary ----
def summarize_run(run: RunResult) -> str:
    lines = [
        f"Seed: {run.seed}",
        f"Total Steps: {run.total_steps}",
        f"Terminals: {run.terminals_reached}",
    ]
    if run.episodes:
        avg_reward = sum(e.total_reward for e in run.episodes) / len(run.episodes)
        avg_len = sum(e.steps for e in run.episodes) / len(run.episodes)
        avg_manh = sum(e.avg_manhattan for e in run.episodes) / len(run.episodes)
        lines += [
            f"Avg Reward/episode: {avg_reward:.2f}",
            f"Avg Steps/episode: {avg_len:.1f}",
            f"Avg Manhattan/step: {avg_manh:.2f}",
        ]
    return "\n".join(lines)

if __name__ == "__main__":
    # smoke test (comment out if you prefer)
    r1a, r1b = two_runs_exp1_pgreedy()
    print("=== Exp1(c) PGREEDY ===")
    print(summarize_run(r1a)); print(summarize_run(r1b))
/home/wstewart/Desktop/COSC4368-task4/README.md
# COSC4368-task4

```
python3 main.py --exp 1b
python3 main.py --exp 1c
python3 main.py --exp 1d
python3 main.py --exp 2
python3 main.py --exp 3q
python3 main.py --exp 3sarsa
python3 main.py --exp 4q
python3 main.py --exp 4sarsa
```

/home/wstewart/Desktop/COSC4368-task4/__pycache__/q_learning.cpython-313.pyc
ï¿½

    ï¿½ï¿½iï¿½  ï¿½                   ï¿½ï¿½   ï¿½ S r SSKrSSKrSSKJrJrJr  S rS\S\S\	S\
4S	 jrS\S\S\	S
\
4S jrS\S\S\\	   S
\	4S
 jr
S\S\S\\	   S
\
4S jrS\S\	S
\4S jrS\S\S\	S\
S\S\\	   S\
S\
4S jrg)a  
Everything in this file is related to Q_learning
contains the algorithm and the means to update tables

for reference:
State format: (i, j, i', j', x, x', a, b, c, d, e, f)
    - (i,j): Female agent position
    - (i',j'): Male agent position
    - x: Female carrying block (0 or 1)
    - x': Male carrying block (0 or 1)
    - a: blocks in dropoff (1,1)
    - b: blocks in dropoff (1,5)
    - c: blocks in dropoff (3,3)
    - d: blocks in pickup (3,5)
    - e: blocks in pickup (4,2)
    - f: blocks in dropoff (5,5)
ï¿½    N)ï¿½Tupleï¿½Setï¿½Dictc                  ï¿½   ï¿½ 0 $ )Nï¿½ r   ï¿½    ï¿½3/home/wstewart/Desktop/COSC4368-task4/q_learning.pyï¿½create_q_tabler
      s   ï¿½ ï¿½
ï¿½Ir   ï¿½q_tableï¿½stateï¿½actionï¿½valuec                 ï¿½   ï¿½ X4nX0U'   g)zm
q_table: Dictionary storing Q-values
state: Simplified state tuple
action: Action string
value: New Q-value
Nr   )r   r   r
   r   ï¿½keys        r	   ï¿½set_q_valuesr      s   ï¿½ ï¿½ ï¿½/ï¿½Cï¿½ï¿½Cï¿½Lr   ï¿½returnc                 ï¿½,   ï¿½ X4nU R                  US5      $ )zv
q_table: Dictionary storing q-values
state: Simplified state tuple (i, j, x, delta_i, delta_j)
action: Action string
ï¿½        )ï¿½get)r   r   r
   r   s       r	   ï¿½get_q_valuesr   &   s   ï¿½ ï¿½ ï¿½/ï¿½Cï¿½ï¿½;ï¿½;ï¿½sï¿½Cï¿½ ï¿½ r   ï¿½applicable_actionsc                 ï¿½ï¿½   ï¿½ 0 nU H  n[        XU5      nXSU'   M     [        UR                  5       5      n/ nUR                  5        H  u  pEXV:X  d  M  UR	                  U5        M     [
        R                  " U5      $ )zï¿½
Return action with highest Q-value from applicable actions.
Breaks ties randomly.

q_table: Dictionary storing Q-values
state: Simplified state tuple
applicable_actions: Set of valid actions
)r   ï¿½maxï¿½valuesï¿½itemsï¿½appendï¿½randomï¿½choice)r   r   r   ï¿½
action_q_valsr
   ï¿½q_valueï¿½	max_valueï¿½best_actions           r	   ï¿½get_best_actionr#   0   s}   ï¿½ ï¿½ ï¿½Mï¿½$ï¿½ï¿½ï¿½wï¿½vï¿½6ï¿½ï¿½ 'ï¿½fï¿½ï¿½ %ï¿½
 ï¿½Mï¿½(ï¿½(ï¿½*ï¿½+ï¿½Iï¿½ ï¿½Kï¿½(ï¿½.ï¿½.ï¿½0ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½vï¿½&ï¿½ 1ï¿½ ï¿½=ï¿½=ï¿½ï¿½%ï¿½%r   c                 ï¿½x   ï¿½ U(       d  g/ nU H   n[        XU5      nUR                  U5        M"     [        U5      $ )zï¿½
Return the highest Q-value among applicable actions.
this function is for returning the number for q-learning formula

q_table: Dictionary storing Q-values
state: Simplified state tuple
applicable_actions: Set of valid actions

r   )r   r   r   )r   r   r   ï¿½q_valuesr
   r    s         r	   ï¿½get_max_q_valuer&   K   s?   ï¿½ ï¿½ ï¿½ï¿½ ï¿½Hï¿½$ï¿½ï¿½ï¿½wï¿½vï¿½6ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ %ï¿½
 ï¿½xï¿½=ï¿½r   ï¿½
full_stateï¿½agentc                 ï¿½T   ï¿½ U u  p#pEpgpï¿½pï¿½pï¿½US:X  a
  X$-
  nX5-
  nX#XnU4$ XB-
  nXS-
  nXEX~U4$ )ax  
simplify the state to only use what is needed for the q-learning algorithm
refer to the top of this code to see what the full state is and what variables mean what
simplified state space : (i, j, x, delta_i, delta_j) is the end result
    - (i, j) = agent position
    - x = agent carrying block (boolean)
    - delta_i, delta_j = Relative row/column position to other agent
ï¿½Fr   )r'   r(   ï¿½iï¿½jï¿½i_primeï¿½j_primeï¿½xï¿½x_primeï¿½aï¿½bï¿½cï¿½dï¿½eï¿½fï¿½delta_iï¿½delta_js                   r	   ï¿½simplify_stater9   b   sV   ï¿½ ï¿½ <Fï¿½8ï¿½Aï¿½'ï¿½Aï¿½ï¿½aï¿½Aï¿½ 
ï¿½ï¿½|ï¿½ï¿½+ï¿½ï¿½ï¿½+ï¿½ï¿½ï¿½Qï¿½ï¿½(ï¿½(ï¿½ï¿½+ï¿½ï¿½ï¿½+ï¿½ï¿½ï¿½ï¿½7ï¿½:ï¿½:r   ï¿½rewardï¿½
next_stateï¿½applicable_next_actionsï¿½alphaï¿½gammac                 ï¿½p   ï¿½ [        XU5      n[        XU5      n	SU-
  U-  XcXy-  -   -  -   n
[        XX*5        g)u  
Formula: Q(s,a) â† (1-Î±)*Q(s,a) + Î±*[R + Î³*max Q(s',a')]

q_table: Dictionary storing Q-values
    state: Current simplified state
    action: Action taken
    reward: Immediate reward received
    next_state: Next simplified state
    applicable_next_actions: Set of valid actions in next state
    alpha: Learning rate (0 < Î± â‰¤ 1)
    gamma: Discount factor (0 â‰¤ Î³ â‰¤ 1)

    note from slides:
        "aâ€™ has to be an applicable operator in sâ€™; e.g. pickup and drop-off are not
        applicable in a pickup/dropoff states if empty/full! The q-values of
        non-applicable operators are therefore not considered! "
ï¿½   N)r   r&   r   )r   r   r
   r:   r;   r<   r=   r>   ï¿½old_qï¿½
max_next_qï¿½new_qs              r	   ï¿½update_q_learningrD   w   sK   ï¿½ ï¿½2 
ï¿½ï¿½ï¿½0ï¿½Eï¿½ !ï¿½ï¿½6Mï¿½Nï¿½Jï¿½ ï¿½ï¿½Yï¿½%ï¿½ï¿½%ï¿½Eï¿½4Fï¿½+Fï¿½"Gï¿½Gï¿½Eï¿½ ï¿½ï¿½ï¿½/r   )ï¿½__doc__ï¿½numpyï¿½npr   ï¿½typingr   r   r   r
   ï¿½strï¿½floatr   r   r#   r&   r9   rD   r   r   r	   ï¿½<module>rK      s  ï¿½ï¿½ï¿½$ ï¿½ 
ï¿½ #ï¿½ #ï¿½ï¿½	ï¿½$ï¿½ 	ï¿½uï¿½ 	ï¿½cï¿½ 	ï¿½%ï¿½ 	ï¿½!ï¿½$ï¿½ !ï¿½uï¿½ !ï¿½cï¿½ !ï¿½eï¿½ !ï¿½&ï¿½Tï¿½ &ï¿½%ï¿½ &ï¿½Sï¿½ï¿½Xï¿½ &ï¿½RUï¿½ &ï¿½6ï¿½Tï¿½ ï¿½%ï¿½ ï¿½Sï¿½ï¿½Xï¿½ ï¿½RWï¿½ ï¿½.;ï¿½uï¿½ ;ï¿½Sï¿½ ;ï¿½Uï¿½ ;ï¿½*"0ï¿½tï¿½ "0ï¿½!ï¿½"0ï¿½ ï¿½"0ï¿½ #ï¿½"0ï¿½ "'ï¿½	"0ï¿½
 /2ï¿½#ï¿½hï¿½"0ï¿½ "ï¿½
"0ï¿½ "ï¿½"0r   
/home/wstewart/Desktop/COSC4368-task4/__pycache__/experiments.cpython-313.pyc
ï¿½

    ï¿½iï¿½'  ï¿½                   ï¿½.  ï¿½ S SK r S SKrS SKJr  S SKJrJrJrJrJ	r	J
r
Jr  S SKr
S SKr\r\\\\4   \4   rS\S\S\S\4S jrS\S\S\S	\SS4
S
 jrS\S\S\
\   S\4S
 jrS\S\S\S\S\S\S\S\
\   SS4S jrS\S\S\S\S\S\	\   S\S\SS4S jrS\S\4S jr\ " S S5      5       r\ " S S5      5       rS\S\S\S\S\S\S\S\4S  jrS!\S\S\4S" jrS\S\4S# jrS5S\S$\S\4S% jjr S5S\S$\S\4S& jjr!S'\\/\4   S(\S)\4S* jr"S6S(\S)\4S+ jjr#S6S(\S)\4S, jjr$S6S(\S)\4S- jjr%S7S(\S)\4S. jjr&S8S$\S(\S)\4S/ jjr'S9S$\S(\S)\4S0 jjr(S1\S\4S2 jr)\*S3:X  a/  \$" 5       u  r+r,\-" S45        \-" \)" \+5      5        \-" \)" \,5      5        gg):ï¿½    N)ï¿½	dataclass)ï¿½Anyï¿½Callableï¿½Dictï¿½Listï¿½Optionalï¿½Sequenceï¿½Tupleï¿½Qï¿½sï¿½aï¿½returnc                 ï¿½(   ï¿½ U R                  X4S5      $ )Nï¿½        )ï¿½get)r   r   r
   s      ï¿½4/home/wstewart/Desktop/COSC4368-task4/experiments.pyï¿½q_getr      s   ï¿½ ï¿½ï¿½5ï¿½5ï¿½!ï¿½ï¿½ï¿½ï¿½ï¿½    ï¿½vc                 ï¿½   ï¿½ X0X4'   g ï¿½Nï¿½ )r   r   r
   r   s       r   ï¿½q_setr      s   ï¿½ ï¿½ï¿½qï¿½fï¿½Ir   ï¿½stateï¿½actionsc                 ï¿½  ï¿½ [         R                  * n/ nU H2  n[        XU5      nXc:ï¿½  a  Uo5/nM  Xc:X  d  M!  UR                  U5        M4     U(       a  [        R
                  " U5      $ [        R
                  " [
        U5      5      $ r   )ï¿½mathï¿½infr   ï¿½appendï¿½randomï¿½choiceï¿½list)r   r   r   ï¿½best_qï¿½bestr
   ï¿½qs          r   ï¿½argmax_qr&      sk   ï¿½ ï¿½ï¿½hï¿½hï¿½Yï¿½Fï¿½
ï¿½Dï¿½
ï¿½ï¿½ï¿½!ï¿½Aï¿½ï¿½ï¿½ï¿½:ï¿½ï¿½Fï¿½sï¿½ï¿½
ï¿½[ï¿½ï¿½Kï¿½Kï¿½ï¿½Nï¿½ ï¿½ #'ï¿½6ï¿½=ï¿½=ï¿½ï¿½ï¿½Hï¿½Fï¿½Mï¿½Mï¿½$ï¿½wï¿½-ï¿½,Hï¿½Hr   ï¿½rï¿½s_nextï¿½alphaï¿½gammaï¿½next_actionsc                 ï¿½   ^ ^ï¿½ U(       a  X6[        U U4S jU 5       5      -  -   OUnSU-
  [        T X5      -  XX-  -   n	[        T XU	5        g )Nc              3   ï¿½>   >#   ï¿½ U  H  n[        TTU5      v ï¿½  M     g 7fr   )r   )ï¿½.0ï¿½anr   r(   s     ï¿½ï¿½r   ï¿½	<genexpr>ï¿½#update_qlearning.<locals>.<genexpr>)   s   ï¿½ï¿½ ï¿½ ï¿½Jï¿½\ï¿½rï¿½eï¿½Aï¿½vï¿½rï¿½2ï¿½2ï¿½\ï¿½s   ï¿½ï¿½   )ï¿½maxr   r   )
r   r   r
   r'   r(   r)   r*   r+   ï¿½targetï¿½new_qs
   `   `     r   ï¿½update_qlearningr6   '   sG   ï¿½ï¿½ ï¿½O[ï¿½Qï¿½#ï¿½Jï¿½\ï¿½Jï¿½Jï¿½Jï¿½
Kï¿½abï¿½Fï¿½
ï¿½ï¿½Yï¿½%ï¿½ï¿½1ï¿½.ï¿½(ï¿½5ï¿½>ï¿½9ï¿½Eï¿½	ï¿½!ï¿½Qï¿½5ï¿½r   ï¿½a_nextc                 ï¿½v   ï¿½ X5b  U[        XU5      -  OS-   nSU-
  [        XU5      -  Xh-  -   n	[        XX)5        g )Nr   r2   )r   r   )
r   r   r
   r'   r(   r7   r)   r*   r4   r5   s
             r   ï¿½update_sarsar9   -   sB   ï¿½ ï¿½
ï¿½6Hï¿½%ï¿½%ï¿½ï¿½6ï¿½2ï¿½2ï¿½cï¿½
Rï¿½Fï¿½
ï¿½ï¿½Yï¿½%ï¿½ï¿½aï¿½.ï¿½(ï¿½5ï¿½>ï¿½9ï¿½Eï¿½	ï¿½!ï¿½ï¿½r   c                 ï¿½T   ï¿½ [        U S   U S   -
  5      [        U S   U S   -
  5      -   $ )Nr   ï¿½   r2   ï¿½   )ï¿½absï¿½r   s    r   ï¿½manhattan_between_agentsr?   4   s/   ï¿½ ï¿½ï¿½qï¿½ï¿½tï¿½aï¿½ï¿½dï¿½{ï¿½ï¿½cï¿½!ï¿½Aï¿½$ï¿½ï¿½1ï¿½ï¿½+ï¿½.ï¿½.ï¿½.r   c                   ï¿½4   ï¿½ \ rS rSr% \\S'   \\S'   \\S'   Srg)ï¿½
EpisodeLogï¿½8   ï¿½stepsï¿½total_rewardï¿½
avg_manhattanr   N)ï¿½__name__ï¿½
__module__ï¿½__qualname__ï¿½__firstlineno__ï¿½intï¿½__annotations__ï¿½floatï¿½__static_attributes__r   r   r   rA   rA   8   s   ï¿½ ï¿½ï¿½Jï¿½ï¿½ï¿½ï¿½r   rA   c                   ï¿½N   ï¿½ \ rS rSr% \\S'   \\S'   \\S'   \\   \S'   \\S'   Sr	g)	ï¿½	RunResultï¿½>   ï¿½seedï¿½total_stepsï¿½terminals_reachedï¿½episodesr   r   N)
rF   rG   rH   rI   rJ   rK   r   rA   ï¿½QTablerM   r   r   r   rO   rO   >   s#   ï¿½ ï¿½

ï¿½Iï¿½ï¿½ï¿½ï¿½ï¿½ï¿½:ï¿½ï¿½ï¿½
ï¿½Ir   rO   rQ   rR   ï¿½warmup_stepsï¿½learnerï¿½schedule_after_warmupc           
      ï¿½  ^^^ï¿½ [         R                  " U 5        [        R                  " U 5      n0 nSn	SmSn
U/nSnSn
Sn/ nUUU4S jnTU:  Gam  [        R                  " Xy5      nU(       d  U	S:X  a  SOSn	M0  U" 5       nU" UUU5      n[        R
                  " UUU	5      u  nnnTS-
  mU
S-
  n
UU-
  nU[
        U5      -
  nUR                  U5        US:X  a0  [        R                  " UU	S:X  a  SOS5      n[        Xï¿½UUUX4U5        OG[        R                  " UU	S:X  a  SOS5      nS nU(       a  U" 5       " UUU5      n[        Xï¿½UUUUX45        UnU	S:X  a  SOSn	U(       aP  U
S-
  n
U[        SU
5      -  nUR                  [        Xï¿½US95        [        R                  " X
-   5      nU/nSnSn
SnSn	TU:  a  GMm  [        U TXï¿½US	9$ )
Nï¿½Fr   r   c                  ï¿½ï¿½   >ï¿½ TT:  a  [         R                  $ [         R                  [         R                  [         R                  S.R	                  T [         R                  5      $ )N)ï¿½prandomï¿½pgreedyï¿½pexploit)ï¿½Pr\   r]   r^   r   )rX   ï¿½
step_countrV   s   ï¿½ï¿½ï¿½r   ï¿½pick_policyï¿½rollout.<locals>.pick_policy^   sC   ï¿½ï¿½ ï¿½ï¿½ï¿½$ï¿½ï¿½9ï¿½9ï¿½ï¿½ï¿½9ï¿½9ï¿½ï¿½ï¿½ï¿½ï¿½
ï¿½
ï¿½Sï¿½Wï¿½Wï¿½Xmï¿½opï¿½oxï¿½oxï¿½yï¿½yr   ï¿½Mr2   r%   ï¿½rC   rD   rE   ï¿½rQ   rR   rS   rT   r   )
r    rQ   ï¿½Wï¿½resetï¿½applicable_actionsï¿½stepr?   r   r6   r9   r3   rA   rO   )rQ   rR   rV   r)   r*   rW   rX   r   r   ï¿½
current_agentï¿½	terminalsï¿½episode_statesï¿½episode_rewardsï¿½
episode_stepsï¿½episode_dist_sumrT   ra   r   ï¿½policyï¿½actionï¿½
next_stateï¿½rewardï¿½doner+   ï¿½next_actions_currr7   ï¿½avg_manhr`   s     `   `                    @r   ï¿½rolloutrw   G   sï¿½  ï¿½ï¿½ ï¿½ ï¿½Kï¿½Kï¿½ï¿½ï¿½
ï¿½Gï¿½Gï¿½Dï¿½Mï¿½Eï¿½ï¿½Aï¿½ï¿½Mï¿½ï¿½Jï¿½ï¿½Iï¿½ $)ï¿½'ï¿½Nï¿½ï¿½Oï¿½ï¿½Mï¿½ï¿½ï¿½!#ï¿½Hï¿½zï¿½
 ï¿½{ï¿½
"ï¿½ï¿½&ï¿½&ï¿½uï¿½<ï¿½ï¿½ï¿½#0ï¿½Cï¿½#7ï¿½Cï¿½Sï¿½Mï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½wï¿½ï¿½*ï¿½ï¿½#$ï¿½6ï¿½6ï¿½%ï¿½ï¿½ï¿½#Gï¿½ ï¿½
ï¿½Fï¿½Dï¿½ï¿½aï¿½ï¿½
ï¿½ï¿½ï¿½ï¿½
ï¿½ï¿½6ï¿½!ï¿½ï¿½ï¿½4ï¿½Zï¿½@ï¿½@ï¿½ï¿½ï¿½ï¿½ï¿½jï¿½)ï¿½ï¿½cï¿½>ï¿½ï¿½/ï¿½/ï¿½
ï¿½=ï¿½TWï¿½CWï¿½Cï¿½]`ï¿½aï¿½Lï¿½ï¿½Qï¿½vï¿½vï¿½zï¿½5ï¿½Q]ï¿½^ï¿½ !ï¿½ 4ï¿½ 4ï¿½Zï¿½
ï¿½Y\ï¿½H\ï¿½ï¿½beï¿½ fï¿½ï¿½ï¿½Fï¿½ ï¿½$ï¿½ï¿½zï¿½3Dï¿½aï¿½Hï¿½ï¿½ï¿½ï¿½6ï¿½6ï¿½:ï¿½vï¿½uï¿½Tï¿½ï¿½ï¿½,ï¿½ï¿½3ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½ï¿½Nï¿½Iï¿½'ï¿½#ï¿½aï¿½ï¿½*?ï¿½?ï¿½Hï¿½ï¿½Oï¿½Oï¿½Jï¿½]ï¿½hpï¿½qï¿½rï¿½ï¿½Gï¿½Gï¿½Dï¿½,ï¿½-ï¿½Eï¿½#ï¿½Wï¿½Nï¿½!ï¿½Oï¿½ï¿½Mï¿½ ï¿½ï¿½ï¿½Mï¿½O ï¿½{ï¿½
"ï¿½R ï¿½$ï¿½Jï¿½)ï¿½jkï¿½lï¿½lr   ï¿½variantc           
      ï¿½    ï¿½ [        USSSSSU S9$ )Nï¿½@  ï¿½ï¿½  ï¿½333333ï¿½?ï¿½      ï¿½?r%   ï¿½rQ   rR   rV   r)   r*   rW   rX   ï¿½rw   )rx   rQ   s     r   ï¿½run_experiment1_variantrï¿½   ï¿½   s    ï¿½ ï¿½ï¿½ï¿½$ï¿½Sï¿½ï¿½Cï¿½ï¿½Gï¿½Uï¿½ Ur   c           
      ï¿½    ï¿½ [        U SSSSSSS9$ )Nrz   r{   r|   r}   ï¿½sarsar]   r~   r   )rQ   s    r   ï¿½run_experiment2rï¿½   ï¿½   s!   ï¿½ ï¿½ï¿½ï¿½$ï¿½Sï¿½ï¿½Cï¿½ï¿½PYï¿½[ï¿½ [r   ï¿½basec           
      ï¿½4   ï¿½ US:X  a  SOSn[        U SSSSUSS9$ )	Nr%   rï¿½   rz   r{   g333333ï¿½?gï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½?r]   r~   r   )rQ   rï¿½   rW   s      r   ï¿½run_experiment3rï¿½   ï¿½   s.   ï¿½ ï¿½ï¿½Sï¿½[ï¿½cï¿½gï¿½Gï¿½ï¿½ï¿½$ï¿½Sï¿½ï¿½Tï¿½7ï¿½R[ï¿½]ï¿½ ]r   c           
      ï¿½  ^^ï¿½ [         R                  " U 5        [        R                  " U 5      n0 nSnSmSnSu  pgSmUS:X  a  SOSnU/n	Sn
SnSn/ n
UU4S jnUS	:  Gaï¿½  TS
:  Gaï¿½  [        R                  " X$5      nU(       d  US:X  a  SOSnM7  U" 5       " X/U5      n[        R
                  " UUU5      u  nnnTS-
  mUS-
  nU
U-
  n
U[
        U5      -
  nU	R                  U5        US:X  a0  [        R                  " UUS:X  a  SOS5      n[        X2UUUXgU5        OG[        R                  " UUS:X  a  SOS5      nS nU(       a  U" 5       " UUU5      n[        X2UUUUXg5        UnUS:X  a  SOSnU(       an  US-
  nU[        SU5      -  nU
R                  [        Xï¿½US
95        US:X  a  [        R                  " SS/5        [        R                  " X-   5      nU/n	Sn
SnSnSnUS	:  a	  TS
:  a  GMï¿½  [        U TX]US9$ )NrZ   r   )r|   r}   r{   r%   rï¿½   r   c                  ï¿½P   >ï¿½ T T:  a  [         R                  $ [         R                  $ r   )r_   r\   r]   )r`   rV   s   ï¿½ï¿½r   ra   ï¿½$run_experiment4.<locals>.pick_policyï¿½   s   ï¿½ï¿½ ï¿½ï¿½ï¿½$ï¿½ï¿½9ï¿½9ï¿½ï¿½ï¿½yï¿½yï¿½r   ï¿½   i N  rc   r2   rd   r<   )r2   r;   )ï¿½   ï¿½   re   )r    rQ   rf   rg   rh   ri   r?   r   r6   r9   r3   rA   ï¿½set_pickupsrO   )rQ   rï¿½   r   r   rj   rk   r)   r*   rW   rl   rm   rn   ro   rT   ra   r   rq   rr   rs   rt   r+   ru   r7   rv   r`   rV   s                           @@r   ï¿½run_experiment4rï¿½   ï¿½   s0  ï¿½ï¿½ ï¿½
ï¿½Kï¿½Kï¿½ï¿½ï¿½
ï¿½Gï¿½Gï¿½Dï¿½Mï¿½Eï¿½ï¿½Aï¿½ï¿½Mï¿½ï¿½Jï¿½ï¿½Iï¿½ï¿½Lï¿½Eï¿½ï¿½Lï¿½ï¿½Sï¿½[ï¿½cï¿½gï¿½Gï¿½#(ï¿½'ï¿½Nï¿½ï¿½Oï¿½ï¿½Mï¿½ï¿½ï¿½!#ï¿½Hï¿½ï¿½ ï¿½aï¿½-ï¿½Jï¿½ï¿½.ï¿½ï¿½&ï¿½&ï¿½uï¿½<ï¿½ï¿½ï¿½#0ï¿½Cï¿½#7ï¿½Cï¿½Sï¿½Mï¿½ï¿½ï¿½ï¿½uï¿½qï¿½1ï¿½ï¿½#$ï¿½6ï¿½6ï¿½%ï¿½ï¿½ï¿½#Gï¿½ ï¿½
ï¿½Fï¿½Dï¿½ï¿½aï¿½ï¿½
ï¿½ï¿½ï¿½ï¿½
ï¿½ï¿½6ï¿½!ï¿½ï¿½ï¿½4ï¿½Zï¿½@ï¿½@ï¿½ï¿½ï¿½ï¿½ï¿½jï¿½)ï¿½ï¿½cï¿½>ï¿½ï¿½/ï¿½/ï¿½
ï¿½=ï¿½TWï¿½CWï¿½Cï¿½]`ï¿½aï¿½Lï¿½ï¿½Qï¿½vï¿½vï¿½zï¿½5ï¿½Q]ï¿½^ï¿½ !ï¿½ 4ï¿½ 4ï¿½Zï¿½
ï¿½Y\ï¿½H\ï¿½ï¿½beï¿½ fï¿½ï¿½ï¿½Fï¿½ ï¿½$ï¿½ï¿½zï¿½3Dï¿½aï¿½Hï¿½ï¿½ï¿½ï¿½6ï¿½6ï¿½:ï¿½vï¿½uï¿½Tï¿½ï¿½ï¿½,ï¿½ï¿½3ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½ï¿½Nï¿½Iï¿½'ï¿½#ï¿½aï¿½ï¿½*?ï¿½?ï¿½Hï¿½ï¿½Oï¿½Oï¿½Jï¿½]ï¿½hpï¿½qï¿½rï¿½ï¿½Aï¿½~ï¿½ï¿½
ï¿½
ï¿½vï¿½vï¿½.ï¿½/ï¿½ï¿½Gï¿½Gï¿½Dï¿½,ï¿½-ï¿½Eï¿½#ï¿½Wï¿½Nï¿½!ï¿½Oï¿½ï¿½Mï¿½ ï¿½ï¿½ï¿½Mï¿½Q ï¿½aï¿½-ï¿½Jï¿½ï¿½.ï¿½T ï¿½$ï¿½Jï¿½)ï¿½jkï¿½lï¿½lr   ï¿½	fn_runnerï¿½seed_aï¿½seed_bc                 ï¿½"   ï¿½ U " U5      U " U5      4$ r   r   )rï¿½   rï¿½   rï¿½   s      r   ï¿½two_runsrï¿½   ï¿½   s   ï¿½ ï¿½ï¿½Vï¿½ï¿½iï¿½ï¿½/ï¿½/ï¿½/r   c                 ï¿½   ï¿½ [        S X5      $ )Nc                 ï¿½   ï¿½ [        SU 5      $ )Nr\   ï¿½rï¿½   r>   s    r   ï¿½<lambda>ï¿½'two_runs_exp1_prandom.<locals>.<lambda>ï¿½   ï¿½   ï¿½ ï¿½5ï¿½iï¿½ï¿½Cr   ï¿½rï¿½   ï¿½rï¿½   rï¿½   s     r   ï¿½two_runs_exp1_prandomrï¿½   ï¿½   ï¿½   ï¿½ ï¿½ï¿½Cï¿½Vï¿½Tï¿½Tr   c                 ï¿½   ï¿½ [        S X5      $ )Nc                 ï¿½   ï¿½ [        SU 5      $ )Nr]   rï¿½   r>   s    r   rï¿½   ï¿½'two_runs_exp1_pgreedy.<locals>.<lambda>ï¿½   rï¿½   r   rï¿½   rï¿½   s     r   ï¿½two_runs_exp1_pgreedyrï¿½   ï¿½   rï¿½   r   c                 ï¿½   ï¿½ [        S X5      $ )Nc                 ï¿½   ï¿½ [        SU 5      $ )Nr^   rï¿½   r>   s    r   rï¿½   ï¿½(two_runs_exp1_pexploit.<locals>.<lambda>ï¿½   s   ï¿½ ï¿½5ï¿½jï¿½!ï¿½Dr   rï¿½   rï¿½   s     r   ï¿½two_runs_exp1_pexploitrï¿½   ï¿½   s   ï¿½ ï¿½ï¿½Dï¿½fï¿½Uï¿½Ur   c                 ï¿½"   ï¿½ [        [        X5      $ r   )rï¿½   rï¿½   rï¿½   s     r   ï¿½
two_runs_exp2rï¿½   ï¿½   s   ï¿½ ï¿½ï¿½Oï¿½Vï¿½4ï¿½4r   c                 ï¿½$   ^ ï¿½ [        U 4S jX5      $ )Nc                 ï¿½   >ï¿½ [        U TS9$ ï¿½N)rï¿½   )rï¿½   ï¿½r   rï¿½   s    ï¿½r   rï¿½   ï¿½two_runs_exp3.<locals>.<lambda>ï¿½   ï¿½   ï¿½ï¿½ ï¿½oï¿½aï¿½dï¿½;r   rï¿½   ï¿½rï¿½   rï¿½   rï¿½   s   `  r   ï¿½
two_runs_exp3rï¿½   ï¿½   ï¿½   ï¿½ï¿½ ï¿½ï¿½;ï¿½Vï¿½Lï¿½Lr   c                 ï¿½$   ^ ï¿½ [        U 4S jX5      $ )Nc                 ï¿½   >ï¿½ [        U TS9$ rï¿½   )rï¿½   rï¿½   s    ï¿½r   rï¿½   ï¿½two_runs_exp4.<locals>.<lambda>ï¿½   rï¿½   r   rï¿½   rï¿½   s   `  r   ï¿½
two_runs_exp4rï¿½   ï¿½   rï¿½   r   ï¿½runc                 ï¿½ï¿½  ï¿½ SU R                    3SU R                   3SU R                   3/nU R                  (       aï¿½  [	        S U R                   5       5      [        U R                  5      -  n[	        S U R                   5       5      [        U R                  5      -  n[	        S U R                   5       5      [        U R                  5      -  nUSUS 3S	US
 3SUS 3/-
  nSR
                  U5      $ )
NzSeed: z
Total Steps: zTerminals: c              3   ï¿½8   #   ï¿½ U  H  oR                   v ï¿½  M     g 7fr   )rD   ï¿½r.   ï¿½es     r   r0   ï¿½ summarize_run.<locals>.<genexpr>  s   ï¿½ ï¿½ ï¿½>ï¿½ï¿½Aï¿½ï¿½ï¿½ï¿½ï¿½   ï¿½c              3   ï¿½8   #   ï¿½ U  H  oR                   v ï¿½  M     g 7fr   )rC   rï¿½   s     r   r0   rï¿½     s   ï¿½ ï¿½ ï¿½4ï¿½|ï¿½!ï¿½gï¿½gï¿½|ï¿½rï¿½   c              3   ï¿½8   #   ï¿½ U  H  oR                   v ï¿½  M     g 7fr   )rE   rï¿½   s     r   r0   rï¿½     s   ï¿½ ï¿½ ï¿½=ï¿½ï¿½1ï¿½ï¿½ï¿½ï¿½rï¿½   zAvg Reward/episode: z.2fzAvg Steps/episode: z.1fzAvg Manhattan/step: ï¿½
)rQ   rR   rS   rT   ï¿½sumï¿½lenï¿½join)rï¿½   ï¿½linesï¿½
avg_rewardï¿½avg_lenrv   s        r   ï¿½
summarize_runrï¿½   ï¿½   sï¿½   ï¿½ ï¿½
ï¿½ï¿½ï¿½ï¿½
ï¿½ï¿½
ï¿½ï¿½ï¿½ï¿½(ï¿½)ï¿½
ï¿½cï¿½+ï¿½+ï¿½,ï¿½-ï¿½
ï¿½Eï¿½
 ï¿½|ï¿½|ï¿½ï¿½>ï¿½ï¿½ï¿½ï¿½>ï¿½>ï¿½ï¿½Sï¿½\ï¿½\ï¿½ARï¿½Rï¿½
ï¿½ï¿½4ï¿½sï¿½|ï¿½|ï¿½4ï¿½4ï¿½sï¿½3ï¿½<ï¿½<ï¿½7Hï¿½Hï¿½ï¿½ï¿½=ï¿½ï¿½ï¿½ï¿½=ï¿½=ï¿½ï¿½Cï¿½Lï¿½Lï¿½@Qï¿½Qï¿½ï¿½
ï¿½"ï¿½:ï¿½cï¿½"2ï¿½3ï¿½!ï¿½'ï¿½#ï¿½ï¿½/ï¿½"ï¿½8ï¿½Cï¿½.ï¿½1ï¿½
ï¿½ 	
ï¿½ï¿½
 ï¿½9ï¿½9ï¿½Uï¿½ï¿½r   ï¿½__main__z=== Exp1(c) PGREEDY ===)r%   )ï¿½   ï¿½   )ï¿½   ï¿½   )r%   ï¿½
   ï¿½   )r%   ï¿½   ï¿½   ).r    r   ï¿½dataclassesr   ï¿½typingr   r   r   r   r   r	   r
   ï¿½worldrf   ï¿½policiesr_   ï¿½Stateï¿½strrL   rU   r   r   r&   r6   r9   rJ   r?   rA   rO   rw   rï¿½   rï¿½   rï¿½   rï¿½   rï¿½   rï¿½   rï¿½   rï¿½   rï¿½   rï¿½   rï¿½   rï¿½   rF   ï¿½r1aï¿½r1bï¿½printr   r   r   ï¿½<module>rï¿½      so  ï¿½ï¿½ ï¿½ ï¿½ !ï¿½ Gï¿½ Gï¿½ Gï¿½ ï¿½ ï¿½ 	ï¿½ï¿½	
ï¿½eï¿½Cï¿½ï¿½Hï¿½oï¿½uï¿½$ï¿½	%ï¿½ï¿½ï¿½Vï¿½ ï¿½ï¿½ ï¿½#ï¿½ ï¿½%ï¿½ ï¿½ï¿½Vï¿½ ï¿½ï¿½ ï¿½#ï¿½ ï¿½%ï¿½ ï¿½Dï¿½ ï¿½	Iï¿½ï¿½ 	Iï¿½uï¿½ 	Iï¿½xï¿½ï¿½}ï¿½ 	Iï¿½ï¿½ 	Iï¿½ï¿½ï¿½ ï¿½5ï¿½ ï¿½Sï¿½ ï¿½Uï¿½ ï¿½Eï¿½ ï¿½RWï¿½ ï¿½`eï¿½ ï¿½#+ï¿½Cï¿½=ï¿½ï¿½59ï¿½ï¿½ï¿½Fï¿½ ï¿½uï¿½ ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½ ï¿½xï¿½X[ï¿½}ï¿½ ï¿½ï¿½ï¿½&+ï¿½ï¿½04ï¿½ï¿½/ï¿½ï¿½ /ï¿½#ï¿½ /ï¿½ ï¿½ï¿½ ï¿½ ï¿½ï¿½
 ï¿½ï¿½ ï¿½ ï¿½ï¿½Emï¿½

ï¿½Emï¿½ï¿½Emï¿½ ï¿½Emï¿½ ï¿½	Emï¿½
 ï¿½Emï¿½ ï¿½
Emï¿½ ï¿½Emï¿½ ï¿½Emï¿½PUï¿½Sï¿½ Uï¿½ï¿½ Uï¿½	ï¿½ Uï¿½
[ï¿½#ï¿½ [ï¿½)ï¿½ [ï¿½
]ï¿½#ï¿½ ]ï¿½Sï¿½ ]ï¿½9ï¿½ ]ï¿½Cmï¿½#ï¿½ Cmï¿½Sï¿½ Cmï¿½9ï¿½ Cmï¿½L0ï¿½ï¿½#ï¿½ï¿½	ï¿½!1ï¿½2ï¿½ 0ï¿½Cï¿½ 0ï¿½ï¿½ 0ï¿½Uï¿½#ï¿½ Uï¿½3ï¿½ Uï¿½Uï¿½#ï¿½ Uï¿½3ï¿½ Uï¿½Vï¿½3ï¿½ Vï¿½Cï¿½ Vï¿½5ï¿½#ï¿½ 5ï¿½Cï¿½ 5ï¿½Mï¿½ï¿½ Mï¿½3ï¿½ Mï¿½Sï¿½ Mï¿½Mï¿½ï¿½ Mï¿½3ï¿½ Mï¿½Sï¿½ Mï¿½ï¿½yï¿½ ï¿½Sï¿½ ï¿½" ï¿½zï¿½ï¿½$ï¿½&ï¿½Hï¿½Cï¿½ï¿½	ï¿½
#ï¿½$ï¿½	ï¿½-ï¿½ï¿½
ï¿½ï¿½uï¿½]ï¿½3ï¿½%7ï¿½8ï¿½	 r   
/home/wstewart/Desktop/COSC4368-task4/__pycache__/world.cpython-313.pyc
ï¿½

    ï¿½ï¿½i  ï¿½            
       ï¿½|  ï¿½ % S SK r S SKJrJrJrJr  SrSS/q\\\\4      \	S'   / SQr
\\\\4      \	S'   SrS	r/ S
Qr
SrSrSrS
\4S jrS\S
\4S jrS\S\S
\\\4   4S jrS\S\S
\4S jrS\S
\\\4   4S jrS\S\S
\\   4S jrS\S\S\S
\\\4   4S jrSS\4S jjrS\S\4S jrS\S\S\4S jrS rg)ï¿½    N)ï¿½Tupleï¿½Setï¿½Dictï¿½Listï¿½   )ï¿½   r   )ï¿½   ï¿½   ï¿½PICKUP_LOCATIONS)ï¿½ï¿½   r
   ï¿½r
   r   ï¿½r   r   ï¿½r   r   ï¿½DROPOFF_LOCATIONSï¿½
   )ï¿½northï¿½southï¿½eastï¿½westï¿½pickupï¿½dropoffï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
   ï¿½returnc                  ï¿½   ï¿½ g)zï¿½
Initial state:
  F at (1,3), M at (5,3), both not carrying;
  dropoffs a,b,c,f start at 0;
  pickup-slot counts d,e start at 10 each.
)r
   r   r   r   r   r   r   r   r   r   r   r   ï¿½ r   ï¿½    ï¿½./home/wstewart/Desktop/COSC4368-task4/world.pyï¿½get_initial_stater    +   s   ï¿½ ï¿½ :r   ï¿½statec                 ï¿½   ï¿½ U u            pp4  pU[         :H  =(       a)    U[         :H  =(       a    U[         :H  =(       a	    U[         :H  $ )z5
Terminal when all dropoffs hit capacity: a=b=c=f=5.
)ï¿½DROPOFF_CAPACITY)r!   ï¿½_ï¿½aï¿½bï¿½cï¿½fs         r   ï¿½is_terminal_stater)   6   sV   ï¿½ ï¿½ */ï¿½&ï¿½Aï¿½qï¿½!ï¿½Qï¿½ï¿½1ï¿½ï¿½qï¿½!ï¿½
ï¿½!ï¿½!ï¿½ 
&ï¿½ï¿½%ï¿½%ï¿½
&ï¿½ï¿½%ï¿½%ï¿½
&ï¿½ ï¿½%ï¿½%ï¿½'r   ï¿½agentc                 ï¿½$   ï¿½ U tp#pEnUS:X  a  X#4$ XE4$ )z)
Return (row, col) for agent 'F' or 'M'.
ï¿½Fr   )r!   r*   ï¿½iï¿½jï¿½i_pï¿½j_pr$   s          r   ï¿½get_agent_positionr1   A   s%   ï¿½ ï¿½ ï¿½ï¿½Aï¿½#ï¿½Qï¿½ï¿½cï¿½\ï¿½Aï¿½6ï¿½1ï¿½ï¿½zï¿½1r   c                 ï¿½$   ï¿½ U t      p#pBUS:X  a  U$ U$ )z/
Return carry flag (0/1) for agent 'F' or 'M'.
r,   r   )r!   r*   r$   ï¿½xï¿½x_ps        r   ï¿½get_agent_carryingr5   I   s%   ï¿½ ï¿½ #ï¿½ï¿½Aï¿½qï¿½!ï¿½Qï¿½3ï¿½ï¿½ï¿½ï¿½1ï¿½%ï¿½#ï¿½%r   c                 ï¿½"   ï¿½ U Gt pp4pVnUUUUUUS.$ )z/
Human-readable dict of block counts at sites.
)ï¿½dropoff_1_1ï¿½dropoff_1_5ï¿½dropoff_3_3ï¿½pickup_slot0ï¿½pickup_slot1ï¿½dropoff_5_5r   )r!   r$   r%   r&   r'   ï¿½dï¿½er(   s           r   ï¿½get_block_countsr?   Q   s.   ï¿½ ï¿½ !ï¿½ï¿½Qï¿½1ï¿½ï¿½qï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
ï¿½ r   c                 ï¿½  ï¿½ U u  p#pEpgpï¿½pï¿½pï¿½US:X  a  X#pï¿½UnXEnnOXEpï¿½UnX#nn[        5       nUS:ï¿½  a   US-
  U:X  a  UU:X  d  UR                  S5        U[        :  a   US-   U:X  a  UU:X  d  UR                  S5        U[        :  a   UU:X  a	  US-   U:X  d  UR                  S5        US:ï¿½  a   UU:X  a	  US-
  U:X  d  UR                  S5        US:X  a]  Xï¿½4[        [        S   5      :X  a  US:ï¿½  a  UR                  S5        O.Xï¿½4[        [        S   5      :X  a  US:ï¿½  a  UR                  S5        US:X  aï¿½  Xï¿½4S	:X  a  U[
        :  a  UR                  S
5        U$ Xï¿½4S:X  a  U	[
        :  a  UR                  S
5        U$ Xï¿½4S:X  a  U
[
        :  a  UR                  S
5        U$ Xï¿½4S
:X  a  U
[
        :  a  UR                  S
5        U$ )zï¿½
Applicable operators for agent in current state.
Enforces:
  - 1..GRID_SIZE boundaries
  - no occupying same cell
  - pickup only at pickup cells with stock and not carrying
  - dropoff only at dropoff cells with capacity and carrying
r,   r
   r   r   r   r   r   r   r   r   r   r   r   )ï¿½setï¿½addï¿½	GRID_SIZEï¿½tupler   r#   )r!   r*   r-   r.   r/   r0   r3   r4   r%   r&   r'   r=   r>   r(   ï¿½rï¿½colï¿½carryingï¿½other_rï¿½other_cï¿½opss                       r   ï¿½aploprK   d   sï¿½  ï¿½ ï¿½ 05ï¿½,ï¿½Aï¿½#ï¿½Aï¿½Aï¿½!ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½3ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½3ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Eï¿½Cï¿½ 	ï¿½1ï¿½uï¿½aï¿½!ï¿½eï¿½wï¿½&ï¿½3ï¿½'ï¿½>ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½9ï¿½}ï¿½aï¿½!ï¿½eï¿½wï¿½.ï¿½3ï¿½'ï¿½>ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
ï¿½Yï¿½ï¿½ï¿½Wï¿½ï¿½ï¿½qï¿½ï¿½Gï¿½1Cï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
ï¿½Qï¿½wï¿½ï¿½Wï¿½ï¿½ï¿½qï¿½ï¿½Gï¿½);ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½1ï¿½}ï¿½
ï¿½8ï¿½uï¿½-ï¿½aï¿½0ï¿½1ï¿½1ï¿½aï¿½!ï¿½eï¿½ï¿½Gï¿½Gï¿½Hï¿½ï¿½ï¿½Xï¿½ï¿½/ï¿½ï¿½2ï¿½3ï¿½
3ï¿½ï¿½Aï¿½ï¿½ï¿½Gï¿½Gï¿½Hï¿½ï¿½ ï¿½1ï¿½}ï¿½
ï¿½8ï¿½vï¿½ï¿½!ï¿½&6ï¿½"6ï¿½ï¿½Gï¿½Gï¿½Iï¿½ï¿½ ï¿½Jï¿½ ï¿½Xï¿½ï¿½
ï¿½Aï¿½(8ï¿½$8ï¿½ï¿½Gï¿½Gï¿½Iï¿½ï¿½ ï¿½Jï¿½ ï¿½Xï¿½ï¿½
ï¿½Aï¿½(8ï¿½$8ï¿½ï¿½Gï¿½Gï¿½Iï¿½ï¿½ ï¿½Jï¿½ ï¿½Xï¿½ï¿½
ï¿½Aï¿½(8ï¿½$8ï¿½ï¿½Gï¿½Gï¿½Iï¿½ï¿½ï¿½Jr   ï¿½actionc                 ï¿½  ï¿½ U u  p4pVpxpï¿½pï¿½pï¿½X4nnXVnnXxnnXï¿½Xï¿½Xï¿½4u  nnnnnnUS:X  a  X4nnUnOXVnnUnSnUS:X  a  US:X  a  US-
  nOUS-
  n[         nOï¿½US:X  a  US:X  a  US-   nOUS-   n[         nOï¿½US:X  a  US:X  a  US-   nOUS-   n[         nOï¿½US:X  a  US:X  a  US-
  nOUS-
  n[         nOï¿½US:X  aM  US:X  a  SnOSnUU4[        [        S   5      :X  a  US-
  nOUU4[        [        S   5      :X  a  U
S-
  n[        nONUS	:X  aH  US:X  a  SnOSnUU4S
:X  a  U	S-   nO)UU4S:X  a  U
S-   nOUU4S:X  a  US-   nO
UU4S
:X  a  US-   n[        nUUUUUUUUUUUU4nUU4$ )z^
Apply action for the given agent. Assumes action is applicable.
Returns (new_state, reward).
r,   r   r   r
   r   r   r   r   r   r   r   r   r   )ï¿½REWARD_MOVErD   r   ï¿½
REWARD_PICKUPï¿½REWARD_DROPOFF) r!   rL   r*   r-   r.   r/   r0   r3   r4   r%   r&   r'   r=   r>   r(   ï¿½new_iï¿½new_jï¿½new_i_pï¿½new_j_pï¿½new_xï¿½new_x_pï¿½new_aï¿½new_bï¿½new_cï¿½new_dï¿½new_eï¿½new_frE   rF   rG   ï¿½rewardï¿½	new_states                                    r   ï¿½applyr_   ï¿½   s  ï¿½ ï¿½
 05ï¿½,ï¿½Aï¿½#ï¿½Aï¿½Aï¿½!ï¿½ï¿½ ï¿½5ï¿½Eï¿½ï¿½Wï¿½Gï¿½ï¿½7ï¿½Eï¿½/0ï¿½Qï¿½1ï¿½/?ï¿½,ï¿½Eï¿½5ï¿½%ï¿½ï¿½ï¿½uï¿½ 
ï¿½ï¿½|ï¿½ï¿½3ï¿½ï¿½ï¿½ï¿½ï¿½3ï¿½ï¿½ï¿½ï¿½
ï¿½Fï¿½
ï¿½ï¿½ï¿½ï¿½Cï¿½<ï¿½ï¿½ï¿½Eï¿½Eï¿½ï¿½!ï¿½eï¿½Gï¿½ï¿½ï¿½	ï¿½7ï¿½	ï¿½ï¿½Cï¿½<ï¿½ï¿½ï¿½Eï¿½Eï¿½ï¿½!ï¿½eï¿½Gï¿½ï¿½ï¿½	ï¿½6ï¿½	ï¿½ï¿½Cï¿½<ï¿½ï¿½!ï¿½Gï¿½Eï¿½ï¿½Aï¿½gï¿½Gï¿½ï¿½ï¿½	ï¿½6ï¿½	ï¿½ï¿½Cï¿½<ï¿½ï¿½!ï¿½Gï¿½Eï¿½ï¿½Aï¿½gï¿½Gï¿½ï¿½ï¿½	ï¿½8ï¿½	ï¿½ï¿½Cï¿½<ï¿½ï¿½Eï¿½ï¿½Gï¿½ 
ï¿½sï¿½8ï¿½uï¿½-ï¿½aï¿½0ï¿½1ï¿½1ï¿½ï¿½ï¿½Eï¿½Eï¿½ï¿½ï¿½Xï¿½ï¿½/ï¿½ï¿½2ï¿½3ï¿½
3ï¿½ï¿½ï¿½Eï¿½Eï¿½ï¿½ï¿½	ï¿½9ï¿½	ï¿½ï¿½Cï¿½<ï¿½ï¿½Eï¿½ï¿½Gï¿½
ï¿½sï¿½8ï¿½vï¿½ï¿½ï¿½ï¿½Eï¿½Eï¿½ï¿½ï¿½Xï¿½ï¿½
ï¿½ï¿½ï¿½Eï¿½Eï¿½ï¿½ï¿½Xï¿½ï¿½
ï¿½ï¿½ï¿½Eï¿½Eï¿½ï¿½ï¿½Xï¿½ï¿½
ï¿½ï¿½ï¿½Eï¿½Eï¿½ï¿½ï¿½ 	ï¿½uï¿½gï¿½wï¿½
ï¿½wï¿½ï¿½uï¿½eï¿½
ï¿½uï¿½eï¿½ï¿½Iï¿½
 ï¿½fï¿½ï¿½r   ï¿½seedc                 ï¿½B   ï¿½ [         R                  " U 5        [        5       $ )zJReset world and return the initial state. Seed accepted for compatibility.)ï¿½randomr`   r    )r`   s    r   ï¿½resetrc   ï¿½   s   ï¿½ ï¿½
ï¿½Kï¿½Kï¿½ï¿½ï¿½ï¿½ï¿½r   c                 ï¿½   ï¿½ [        X5      $ )z0Adapter name expected by runners; wraps aplop().)rK   )r!   r*   s     r   ï¿½applicable_actionsre     s
   ï¿½ ï¿½ï¿½ï¿½ï¿½r   c                 ï¿½<   ï¿½ [        XU5      u  p4[        U5      nX4U4$ )zE
One environment step for agent. Returns (next_state, reward, done).
)r_   r)   )r!   rL   r*   ï¿½
next_stater]   ï¿½dones         r   ï¿½stepri   	  s(   ï¿½ ï¿½ ï¿½uï¿½eï¿½4ï¿½ï¿½Jï¿½ï¿½Zï¿½(ï¿½Dï¿½ï¿½tï¿½#ï¿½#r   c                 ï¿½   ï¿½ [        U [        [        45      (       a  [        U 5      S:w  a  [	        S5      e[        U S   5      [        U S   5      /qg)zï¿½
Experiment 4 hook: change pickup coordinates WITHOUT touching d/e counts in the state.
The two slot counts (d,e) always map to PICKUP_LOCATIONS[0] and [1], respectively.
r
   zGset_pickups expects exactly two pickup coordinates, e.g., [(1,2),(4,5)]r   r
   N)ï¿½
isinstanceï¿½listrD   ï¿½lenï¿½
ValueErrorr   )ï¿½pickupss    r   ï¿½set_pickupsrp     sI   ï¿½ ï¿½ ï¿½gï¿½ï¿½eï¿½}ï¿½-ï¿½-ï¿½ï¿½Wï¿½ï¿½ï¿½1Bï¿½ï¿½bï¿½cï¿½cï¿½ï¿½gï¿½aï¿½jï¿½)ï¿½5ï¿½ï¿½ï¿½ï¿½+<ï¿½=ï¿½r   )r   )rb   ï¿½typingr   r   r   r   rC   r   ï¿½intï¿½__annotations__r   r#   ï¿½INITIAL_PICKUP_BLOCKSï¿½ACTIONSrN   rO   rP   r    ï¿½boolr)   ï¿½strr1   r5   r?   rK   r_   rc   re   ri   rp   r   r   r   ï¿½<module>rx      s{  ï¿½ï¿½ ï¿½ )ï¿½ )ï¿½ 
ï¿½	ï¿½ ,2ï¿½6ï¿½*:ï¿½ ï¿½$ï¿½uï¿½Sï¿½#ï¿½Xï¿½ï¿½'ï¿½ :ï¿½ ,Lï¿½ ï¿½4ï¿½ï¿½cï¿½3ï¿½hï¿½ï¿½(ï¿½ Kï¿½ ï¿½ ï¿½ï¿½ ï¿½ Bï¿½ï¿½ ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½:ï¿½5ï¿½ :ï¿½'ï¿½Uï¿½ 'ï¿½tï¿½ 'ï¿½2ï¿½eï¿½ 2ï¿½Cï¿½ 2ï¿½Eï¿½#ï¿½sï¿½(ï¿½Oï¿½ 2ï¿½&ï¿½eï¿½ &ï¿½Cï¿½ &ï¿½Cï¿½ &ï¿½ï¿½Eï¿½ ï¿½dï¿½3ï¿½ï¿½8ï¿½nï¿½ ï¿½&5ï¿½ï¿½ 5ï¿½sï¿½ 5ï¿½sï¿½3ï¿½xï¿½ 5ï¿½xWï¿½ï¿½ Wï¿½ï¿½ Wï¿½Cï¿½ Wï¿½Eï¿½%ï¿½ï¿½*ï¿½4Eï¿½ Wï¿½|ï¿½ï¿½ ï¿½ï¿½eï¿½ ï¿½Cï¿½ ï¿½
$ï¿½ï¿½ $ï¿½sï¿½ $ï¿½3ï¿½ $ï¿½>r   
/home/wstewart/Desktop/COSC4368-task4/__pycache__/policies.cpython-313.pyc
ï¿½

    %ï¿½if  ï¿½                   ï¿½  ï¿½ S SK JrJrJrJr  S SKrS SKr\rS\\   S\\   4S jr	S\\\\4   \
4   S\S\S\
4S	 jrS\\\\4   \
4   S\S\\   S\4S
 jrS\S\\   S\S\4S
 jr
S\S\\   S\S\4S jrSS\S\\   S\S\
S\4
S jjrS\S\\   S\S\4S jrS\S\\   S\S\4S jrSS\S\\   S\S\
S\4
S jjrg)ï¿½    )ï¿½Iterableï¿½Dictï¿½Tupleï¿½ListNï¿½actionsï¿½returnc                 ï¿½@   ï¿½ [        U 5      nU(       d  [        S5      eU$ )Nz No applicable actions available.)ï¿½listï¿½
ValueError)r   ï¿½lsts     ï¿½1/home/wstewart/Desktop/COSC4368-task4/policies.pyï¿½_as_listr      s   ï¿½ ï¿½
ï¿½wï¿½-ï¿½Cï¿½ï¿½ï¿½;ï¿½<ï¿½<ï¿½ï¿½Jï¿½    ï¿½Qï¿½stateï¿½actionc                 ï¿½(   ï¿½ U R                  X4S5      $ )Ng        )ï¿½get)r   r   r   s      r
   ï¿½_qr      s   ï¿½ ï¿½ï¿½5ï¿½5ï¿½%ï¿½ï¿½#ï¿½&ï¿½&r   c                 ï¿½  ï¿½ [        U5      n[        R                  * n/ nU H3  n[        XU5      nXt:ï¿½  a  UnU/nM  Xt:X  d  M"  UR	                  U5        M5     U(       a  [
        R                  " U5      $ [
        R                  " U5      $ ï¿½N)r   ï¿½mathï¿½infr   ï¿½appendï¿½randomï¿½choice)r   r   r   ï¿½actsï¿½best_qï¿½bestï¿½aï¿½qs           r
   ï¿½_best_actionr"      ss   ï¿½ ï¿½ï¿½Gï¿½ï¿½Dï¿½ï¿½hï¿½hï¿½Yï¿½Fï¿½ï¿½Dï¿½
ï¿½ï¿½ï¿½qï¿½ï¿½Oï¿½ï¿½ï¿½:ï¿½ï¿½Fï¿½ï¿½3ï¿½Dï¿½
ï¿½[ï¿½ï¿½Kï¿½Kï¿½ï¿½Nï¿½
 ï¿½ #'ï¿½6ï¿½=ï¿½=ï¿½ï¿½ï¿½?ï¿½Fï¿½Mï¿½Mï¿½$ï¿½,?ï¿½?r   ï¿½applicable_actionsï¿½q_tablec                 ï¿½`   ï¿½ [        U5      nSU;   a  gSU;   a  g[        R                  " U5      $ )zN
PRANDOM: If pickup or dropoff is applicable, do it; otherwise choose random.
ï¿½pickupï¿½dropoff)r   r   r   ï¿½r   r#   r$   r   s       r
   ï¿½prandomr)   #   s4   ï¿½ ï¿½ ï¿½&ï¿½'ï¿½Dï¿½ï¿½4ï¿½ï¿½ï¿½ï¿½Dï¿½ï¿½ï¿½ï¿½=ï¿½=ï¿½ï¿½ï¿½r   c                 ï¿½L   ï¿½ [        U5      nSU;   a  gSU;   a  g[        X U5      $ )zc
PGREEDY: If pickup/dropoff is applicable, do it; otherwise choose argmax-Q (break ties randomly).
r&   r'   )r   r"   r(   s       r
   ï¿½pgreedyr+   .   s2   ï¿½ ï¿½ ï¿½&ï¿½'ï¿½Dï¿½ï¿½4ï¿½ï¿½ï¿½ï¿½Dï¿½ï¿½ï¿½ï¿½ï¿½ï¿½-ï¿½-r   ï¿½epsilonc                 ï¿½   ï¿½ [        U5      nSU;   a  gSU;   a  g[        R                  " 5       U:  a  [        R                  " U5      $ [        X U5      $ )zï¿½
PEXPLOIT: If pickup/dropoff is applicable, do it; otherwise:
  - with probability (1 - epsilon) pick greedy action
  - with probability epsilon pick a random applicable action
Default epsilon=0.2 matches spec's 80/20 split.
r&   r'   )r   r   r   r"   )r   r#   r$   r,   r   s        r
   ï¿½pexploitr.   9   sO   ï¿½ ï¿½ ï¿½&ï¿½'ï¿½Dï¿½ï¿½4ï¿½ï¿½ï¿½ï¿½Dï¿½ï¿½ï¿½
ï¿½}ï¿½}ï¿½ï¿½ï¿½ ï¿½ï¿½}ï¿½}ï¿½Tï¿½"ï¿½"ï¿½ï¿½ï¿½ï¿½-ï¿½-r   c                 ï¿½   ï¿½ [        XU5      $ r   )r)   ï¿½r   r#   r$   s      r
   ï¿½PRandomr1   M   ï¿½   ï¿½ ï¿½ï¿½5ï¿½gï¿½6ï¿½6r   c                 ï¿½   ï¿½ [        XU5      $ r   )r+   r0   s      r
   ï¿½PGreedyr4   P   r2   r   c                 ï¿½   ï¿½ [        XX#S9$ )N)r,   )r.   )r   r#   r$   r,   s       r
   ï¿½PExploitr6   S   s   ï¿½ ï¿½ï¿½Eï¿½wï¿½Hï¿½Hr   )gï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½?)ï¿½typingr   r   r   r   r   r   ï¿½Stateï¿½strr   ï¿½floatr   r"   r)   r+   r.   r1   r4   r6   ï¿½ r   r
   ï¿½<module>r<      sï¿½  ï¿½ï¿½ .ï¿½ .ï¿½ 
ï¿½ ï¿½
ï¿½ï¿½ï¿½hï¿½sï¿½mï¿½ ï¿½ï¿½Sï¿½	ï¿½ ï¿½'ï¿½$ï¿½uï¿½Uï¿½Cï¿½Zï¿½ ï¿½%ï¿½'ï¿½
(ï¿½ 'ï¿½ï¿½ 'ï¿½ï¿½ 'ï¿½ï¿½ 'ï¿½@ï¿½Dï¿½ï¿½uï¿½cï¿½zï¿½*ï¿½Eï¿½1ï¿½2ï¿½ @ï¿½5ï¿½ @ï¿½8ï¿½TWï¿½=ï¿½ @ï¿½]`ï¿½ @ï¿½"	ï¿½5ï¿½ 	ï¿½hï¿½sï¿½mï¿½ 	ï¿½dï¿½ 	ï¿½sï¿½ 	ï¿½	.ï¿½5ï¿½ 	.ï¿½hï¿½sï¿½mï¿½ 	.ï¿½dï¿½ 	.ï¿½sï¿½ 	.ï¿½.ï¿½Eï¿½ .ï¿½xï¿½ï¿½}ï¿½ .ï¿½tï¿½ .ï¿½V[ï¿½ .ï¿½fiï¿½ .ï¿½(7ï¿½5ï¿½ 7ï¿½hï¿½sï¿½mï¿½ 7ï¿½dï¿½ 7ï¿½sï¿½ 7ï¿½7ï¿½5ï¿½ 7ï¿½hï¿½sï¿½mï¿½ 7ï¿½dï¿½ 7ï¿½sï¿½ 7ï¿½Iï¿½Eï¿½ Iï¿½xï¿½ï¿½}ï¿½ Iï¿½tï¿½ Iï¿½V[ï¿½ Iï¿½fiï¿½ Ir   
/home/wstewart/Desktop/COSC4368-task4/__pycache__/visualize.cpython-313.pyc
ï¿½

    ï¿½i  ï¿½                   ï¿½ï¿½  ï¿½ S SK JrJrJrJrJrJr  S SKrS SKJ	r
  \\\4   r\r
SrS\
S\\\4   4S jrS\S\4S	 jr\S
S\S4S\
S
\\\4   S\\   S\\   S\\
/\\\4   4   S\4S jjr\S
S\S4S\\
   S
\\\4   S\\   S\\   S\\
/\\\4   4   S\4S jjr\S4S\\
   S\\
/\\\4   4   S\4S jjrS\S\\\4   4S jr\S\SS4S\\\\4   \4   S
\\\4   S\S\\
/\\\4   4   S\S\4S  jjrg)!ï¿½    )ï¿½Dictï¿½Iterableï¿½Listï¿½Tupleï¿½Callableï¿½AnyNï¿½ï¿½   r
   ï¿½stateï¿½returnc                 ï¿½r   ï¿½ [        U S   5      [        U S   5      4[        U S   5      [        U S   5      44$ )Nr   ï¿½   ï¿½   ï¿½   )ï¿½int)r   s    ï¿½2/home/wstewart/Desktop/COSC4368-task4/visualize.pyï¿½extract_positions_from_stater      s7   ï¿½ ï¿½ï¿½ï¿½aï¿½ï¿½Mï¿½3ï¿½uï¿½Qï¿½xï¿½=ï¿½)ï¿½Cï¿½ï¿½aï¿½ï¿½Mï¿½3ï¿½uï¿½Qï¿½xï¿½=ï¿½+Iï¿½Iï¿½Iï¿½    ï¿½widthï¿½heightc                 ï¿½v  ï¿½ U R                  SUS-   5        U R                  SUS-   5        U R                  [        SUS-   5      5        U R	                  [        SUS-   5      5        U R                  S5        U R
                  SSS9  U R                  5         U R                  S5        U R                  S5        g )	Ng      ï¿½?r   Tï¿½equalï¿½box)ï¿½
adjustableï¿½colï¿½row)
ï¿½set_xlimï¿½set_ylimï¿½
set_xticksï¿½rangeï¿½
set_yticksï¿½gridï¿½
set_aspectï¿½invert_yaxisï¿½
set_xlabelï¿½
set_ylabel)ï¿½axr   r   s      r   ï¿½
_draw_gridr(      sï¿½   ï¿½ ï¿½ï¿½Kï¿½Kï¿½ï¿½Uï¿½Sï¿½[ï¿½!ï¿½ï¿½Kï¿½Kï¿½ï¿½Vï¿½cï¿½\ï¿½"ï¿½ï¿½Mï¿½Mï¿½%ï¿½ï¿½5ï¿½1ï¿½9ï¿½%ï¿½&ï¿½ï¿½Mï¿½Mï¿½%ï¿½ï¿½6ï¿½Aï¿½:ï¿½&ï¿½'ï¿½ï¿½Gï¿½Gï¿½Dï¿½Mï¿½ï¿½Mï¿½Mï¿½'ï¿½eï¿½Mï¿½,ï¿½ï¿½Oï¿½Oï¿½ï¿½ï¿½Mï¿½Mï¿½%ï¿½ï¿½ï¿½Mï¿½Mï¿½%ï¿½r   ))r   r
   )ï¿½   r   ))r   r   )r   r
   )r   r   r	   zPD-World snapshotï¿½	grid_sizeï¿½pickup_cellsï¿½
dropoff_cellsï¿½position_extractorï¿½titlec           	      ï¿½  ï¿½ Uu  pgU" U 5      u  pï¿½[         R                  " 5       u  pï¿½[        Xï¿½U5        U H  u  pï¿½UR                  U/U
/SSSS9  M     U H  u  pï¿½UR                  U/U
/SSSS9  M     UR                  US   /US   /S	SS
S9  UR                  U	S   /U	S   /SSSS9  UR	                  S
S9  UR                  U5        [         R                  " 5         g )Nï¿½sï¿½ï¿½   ï¿½pickup)ï¿½markerr0   ï¿½labelï¿½Dï¿½dropoffr   r   ï¿½oï¿½Fï¿½^ï¿½Mï¿½upper rightï¿½ï¿½loc)ï¿½pltï¿½subplotsr(   ï¿½scatterï¿½legendï¿½	set_titleï¿½show)r   r*   r+   r,   r-   r.   r   r   ï¿½f_posï¿½m_posï¿½figr'   ï¿½xï¿½ys                 r   ï¿½
plot_worldrI      sï¿½   ï¿½ ï¿½ ï¿½Mï¿½Eï¿½%ï¿½eï¿½,ï¿½Lï¿½Eï¿½ï¿½lï¿½lï¿½nï¿½Gï¿½Cï¿½ï¿½rï¿½&ï¿½!ï¿½ï¿½ï¿½ï¿½
ï¿½
ï¿½
ï¿½Aï¿½3ï¿½ï¿½ï¿½Cï¿½3ï¿½hï¿½
ï¿½?ï¿½ ï¿½ï¿½ï¿½ï¿½
ï¿½
ï¿½
ï¿½Aï¿½3ï¿½ï¿½ï¿½Cï¿½3ï¿½iï¿½
ï¿½@ï¿½  ï¿½ ï¿½Jï¿½Jï¿½ï¿½aï¿½ï¿½zï¿½Eï¿½!ï¿½Hï¿½:ï¿½cï¿½Sï¿½ï¿½Jï¿½Dï¿½ï¿½Jï¿½Jï¿½ï¿½aï¿½ï¿½zï¿½Eï¿½!ï¿½Hï¿½:ï¿½cï¿½Sï¿½ï¿½Jï¿½Dï¿½ï¿½Iï¿½Iï¿½-ï¿½Iï¿½ ï¿½ï¿½Lï¿½Lï¿½ï¿½ï¿½ï¿½Hï¿½Hï¿½Jr   zEpisode trace (agent paths)ï¿½statesc                 ï¿½$  ï¿½ Uu  pg[         R                  " 5       u  pï¿½[        Xï¿½U5        / n
/ nU  H/  nU" U5      u  pï¿½U
R                  U
5        UR                  U5        M1     U H  u  nnU	R	                  U/U/SSS9  M     U H  u  nnU	R	                  U/U/SSS9  M     [        U
5      S:ï¿½  a<  U
 Vs/ s H  nUS   PM
     nnU
 Vs/ s H  nUS   PM
     nnU	R
                  UUSSS	9  [        U5      S:ï¿½  a=  U Vs/ s H  nUS   PM
     nnU Vs/ s H  nUS   PM
     nnU	R
                  UUSS
SS9  U
(       aA  U	R	                  U
S   S   /U
S   S   /S
SS9  U	R	                  U
S   S   /U
S   S   /S
SSS9  U(       aA  U	R	                  US   S   /US   S   /S
SS9  U	R	                  US   S   /US   S   /S
SSS9  U	R                  SS9  U	R                  U5        [         R                  " 5         g s  snf s  snf s  snf s  snf )Nr0   ï¿½x   )r3   r0   r5   r   r   r   zF path)ï¿½	linewidthr4   z--zM path)rM   ï¿½	linestyler4   ï¿½d   zF start)r0   r4   ï¿½ï¿½ï¿½ï¿½ï¿½rG   zF end)r0   r3   r4   zM startzM endr;   r<   )
r>   r?   r(   ï¿½appendr@   ï¿½lenï¿½plotrA   rB   rC   )rJ   r*   r+   r,   r-   r.   r   r   rF   r'   ï¿½f_pathï¿½m_pathr0   rD   rE   rG   rH   ï¿½pï¿½xsï¿½yss                       r   ï¿½plot_episode_tracerY   8   s&  ï¿½ ï¿½ ï¿½Mï¿½Eï¿½ï¿½lï¿½lï¿½nï¿½Gï¿½Cï¿½ï¿½rï¿½&ï¿½!ï¿½ï¿½Fï¿½ï¿½Fï¿½
ï¿½ï¿½)ï¿½!ï¿½,ï¿½ï¿½ï¿½ï¿½
ï¿½
ï¿½eï¿½ï¿½fï¿½mï¿½mï¿½Eï¿½2ï¿½ ï¿½ ï¿½ï¿½ï¿½Aï¿½
ï¿½
ï¿½
ï¿½Aï¿½3ï¿½ï¿½ï¿½Cï¿½3ï¿½
ï¿½/ï¿½ ï¿½ï¿½ï¿½ï¿½Aï¿½
ï¿½
ï¿½
ï¿½Aï¿½3ï¿½ï¿½ï¿½Cï¿½3ï¿½
ï¿½/ï¿½  ï¿½ ï¿½6ï¿½{ï¿½aï¿½ï¿½"ï¿½
#ï¿½Fï¿½qï¿½aï¿½ï¿½dï¿½Fï¿½ï¿½
#ï¿½ï¿½*@ï¿½ï¿½Aï¿½1ï¿½Qï¿½4ï¿½ï¿½Rï¿½*@ï¿½
ï¿½ï¿½ï¿½ï¿½Bï¿½!ï¿½8ï¿½ï¿½4ï¿½
ï¿½6ï¿½{ï¿½aï¿½ï¿½"ï¿½
#ï¿½Fï¿½qï¿½aï¿½ï¿½dï¿½Fï¿½ï¿½
#ï¿½ï¿½*@ï¿½ï¿½Aï¿½1ï¿½Qï¿½4ï¿½ï¿½Rï¿½*@ï¿½
ï¿½ï¿½ï¿½ï¿½Bï¿½!ï¿½tï¿½8ï¿½ï¿½Dï¿½
ï¿½
ï¿½
ï¿½
ï¿½Fï¿½1ï¿½Iï¿½aï¿½Lï¿½>ï¿½Fï¿½1ï¿½Iï¿½aï¿½Lï¿½>ï¿½Sï¿½	ï¿½
ï¿½Jï¿½
ï¿½
ï¿½
ï¿½Fï¿½2ï¿½Jï¿½qï¿½Mï¿½?ï¿½Vï¿½Bï¿½Zï¿½ï¿½]ï¿½Oï¿½sï¿½3ï¿½gï¿½
ï¿½Vï¿½
ï¿½
ï¿½
ï¿½
ï¿½Fï¿½1ï¿½Iï¿½aï¿½Lï¿½>ï¿½Fï¿½1ï¿½Iï¿½aï¿½Lï¿½>ï¿½Sï¿½	ï¿½
ï¿½Jï¿½
ï¿½
ï¿½
ï¿½Fï¿½2ï¿½Jï¿½qï¿½Mï¿½?ï¿½Vï¿½Bï¿½Zï¿½ï¿½]ï¿½Oï¿½sï¿½3ï¿½gï¿½
ï¿½Vï¿½ï¿½Iï¿½Iï¿½-ï¿½Iï¿½ ï¿½ï¿½Lï¿½Lï¿½ï¿½ï¿½ï¿½Hï¿½Hï¿½Jï¿½ï¿½ $ï¿½ï¿½*@ï¿½ï¿½ $ï¿½ï¿½*@s   ï¿½4G>ï¿½	Hï¿½?Hï¿½H
z"Agent Manhattan distance over timec                 ï¿½  ï¿½ / nU  H<  nU" U5      u  u  pVu  pxUR                  [        XW-
  5      [        Xh-
  5      -   5        M>     [        R                  " 5       u  pï¿½U
R	                  [        [
        U5      5      U5        U
R                  S5        U
R                  S5        U
R                  U5        [        R                  " 5         g )Nï¿½StepzManhattan distance)rQ   ï¿½absr>   r?   rS   r    rR   r%   r&   rB   rC   )rJ   r-   r.   ï¿½	distancesr0   ï¿½frï¿½fcï¿½mrï¿½mcrF   r'   s              r   ï¿½plot_distance_over_timerb   b   sï¿½   ï¿½ ï¿½
 ï¿½Iï¿½
ï¿½ï¿½/ï¿½ï¿½2ï¿½ï¿½ï¿½ï¿½(ï¿½2ï¿½ï¿½ï¿½ï¿½ï¿½Rï¿½Wï¿½ï¿½ï¿½Bï¿½Gï¿½ï¿½4ï¿½5ï¿½ ï¿½ ï¿½lï¿½lï¿½nï¿½Gï¿½Cï¿½ï¿½Gï¿½Gï¿½Eï¿½#ï¿½iï¿½.ï¿½!ï¿½9ï¿½-ï¿½ï¿½Mï¿½Mï¿½&ï¿½ï¿½ï¿½Mï¿½Mï¿½&ï¿½'ï¿½ï¿½Lï¿½Lï¿½ï¿½ï¿½ï¿½Hï¿½Hï¿½Jr   ï¿½actionc                 ï¿½<   ï¿½ U S:X  a  gU S:X  a  gU S:X  a  gU S:X  a  gg	)
Nï¿½north)r   ï¿½333333Ó¿ï¿½south)r   ï¿½333333ï¿½?ï¿½west)rf   r   ï¿½east)rh   r   )ï¿½        rk   ï¿½ )rc   s    r   ï¿½_arrow_for_actionrm   s   s-   ï¿½ ï¿½
ï¿½ï¿½ï¿½)ï¿½
ï¿½ï¿½ï¿½)ï¿½
ï¿½ï¿½ï¿½)ï¿½
ï¿½ï¿½ï¿½)ï¿½r   r8   z"Greedy action by cell from Q-tableTï¿½q_tableï¿½	for_agentï¿½show_valuesc           
      ï¿½H  ï¿½ Uu  pg[         R                  " 5       u  pï¿½[        Xï¿½U5        0 n
U R                  5        H<  u  u  pï¿½n
U" U5      u  pï¿½US:X  a  UOUu  nnUU4nUU
;  d
  Xï¿½U   S   :ï¿½  d  M6  Xï¿½4U
U'   M>     U
R                  5        Hh  u  u  nnu  pï¿½[	        U5      u  nnU	R                  UUUUSSS9  U(       d  M6  [        R                  " U
5      (       d  MS  U	R                  UUU
S SSS9  Mj     U	R                  US	U S
3-   5        [         R                  " 5         g )Nr8   r   gï¿½ï¿½ï¿½Qï¿½ï¿½?T)ï¿½
head_widthï¿½length_includes_headz.2fï¿½center)ï¿½haï¿½vaz (agent=ï¿½))r>   r?   r(   ï¿½itemsrm   ï¿½arrowï¿½mathï¿½isfiniteï¿½textrB   rC   )rn   r*   ro   r-   r.   rp   r   r   rF   r'   ï¿½
best_per_cellr   rc   ï¿½qrD   rE   r   r   ï¿½cellï¿½dxï¿½dys                        r   ï¿½
plot_q_arrowsrï¿½   z   s"  ï¿½ ï¿½ ï¿½Mï¿½Eï¿½ï¿½lï¿½lï¿½nï¿½Gï¿½Cï¿½ï¿½rï¿½&ï¿½!ï¿½ 79ï¿½Mï¿½%ï¿½mï¿½mï¿½oï¿½ï¿½ï¿½ï¿½ï¿½)ï¿½%ï¿½0ï¿½ï¿½ï¿½&ï¿½#ï¿½-ï¿½Eï¿½5ï¿½ï¿½ï¿½Sï¿½ï¿½Sï¿½zï¿½ï¿½ï¿½}ï¿½$ï¿½ï¿½$ï¿½,?ï¿½ï¿½,Bï¿½(Bï¿½#)ï¿½+ï¿½Mï¿½$ï¿½ï¿½ .ï¿½ $1ï¿½#6ï¿½#6ï¿½#8ï¿½ï¿½
ï¿½ï¿½cï¿½Kï¿½Vï¿½"ï¿½6ï¿½*ï¿½ï¿½ï¿½Bï¿½
ï¿½ï¿½ï¿½ï¿½cï¿½2ï¿½rï¿½dï¿½ï¿½ï¿½Nï¿½ï¿½;ï¿½4ï¿½=ï¿½=ï¿½ï¿½+ï¿½+ï¿½ï¿½Gï¿½Gï¿½Cï¿½ï¿½ï¿½3ï¿½ï¿½Xï¿½(ï¿½Gï¿½Cï¿½ $9ï¿½ ï¿½Lï¿½Lï¿½ï¿½8ï¿½Iï¿½;ï¿½aï¿½0ï¿½0ï¿½1ï¿½ï¿½Hï¿½Hï¿½Jr   )ï¿½typingr   r   r   r   r   r   rz   ï¿½matplotlib.pyplotï¿½pyplotr>   r   ï¿½GridPosï¿½Stateï¿½DEFAULT_GRIDr   r(   ï¿½strrI   rY   rb   ï¿½floatrm   ï¿½boolrï¿½   rl   r   r   ï¿½<module>rï¿½      s-  ï¿½ï¿½
 >ï¿½ =ï¿½ ï¿½ ï¿½
ï¿½ï¿½Sï¿½ï¿½/ï¿½ï¿½
ï¿½ï¿½ ï¿½ï¿½Jï¿½ï¿½ Jï¿½%ï¿½ï¿½ï¿½8Hï¿½2Iï¿½ Jï¿½	ï¿½#ï¿½ 	ï¿½sï¿½ 	ï¿½ ".ï¿½&6ï¿½'Gï¿½Eaï¿½$ï¿½
ï¿½ï¿½ï¿½ï¿½Sï¿½#ï¿½Xï¿½ï¿½ï¿½ ï¿½7ï¿½#ï¿½ï¿½ ï¿½Gï¿½$ï¿½	ï¿½
 !ï¿½%ï¿½ï¿½%ï¿½ï¿½ï¿½0@ï¿½*Aï¿½!Aï¿½Bï¿½ï¿½ ï¿½
ï¿½8 ".ï¿½&6ï¿½'Gï¿½Eaï¿½.ï¿½
(ï¿½ï¿½ï¿½Kï¿½(ï¿½ï¿½Sï¿½#ï¿½Xï¿½ï¿½(ï¿½ ï¿½7ï¿½#ï¿½(ï¿½ ï¿½Gï¿½$ï¿½	(ï¿½
 !ï¿½%ï¿½ï¿½%ï¿½ï¿½ï¿½0@ï¿½*Aï¿½!Aï¿½Bï¿½(ï¿½ ï¿½
(ï¿½X Fbï¿½5ï¿½ï¿½ï¿½ï¿½Kï¿½ï¿½ ï¿½%ï¿½ï¿½%ï¿½ï¿½ï¿½0@ï¿½*Aï¿½!Aï¿½Bï¿½ï¿½ ï¿½ï¿½"ï¿½cï¿½ ï¿½eï¿½Eï¿½5ï¿½Lï¿½&9ï¿½ ï¿½ ".ï¿½ï¿½Eaï¿½5ï¿½ï¿½
ï¿½
ï¿½%ï¿½ï¿½Sï¿½ï¿½/ï¿½5ï¿½(ï¿½
)ï¿½ï¿½ï¿½Sï¿½#ï¿½Xï¿½ï¿½ï¿½ ï¿½ï¿½ !ï¿½%ï¿½ï¿½%ï¿½ï¿½ï¿½0@ï¿½*Aï¿½!Aï¿½Bï¿½	ï¿½
 ï¿½ï¿½ ï¿½
r   
